# ğŸ“Š PhÃ¢n tÃ­ch Thá»‹ trÆ°á»ng Tuyá»ƒn dá»¥ng IT Viá»‡t Nam

> Há»‡ thá»‘ng thu tháº­p vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u tuyá»ƒn dá»¥ng IT tá»± Ä‘á»™ng vá»›i AI-powered web crawler, data processing pipeline vÃ  interactive dashboard.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Playwright](https://img.shields.io/badge/Playwright-1.40+-green.svg)](https://playwright.dev/python/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
- [TÃ­nh nÄƒng](#-tÃ­nh-nÄƒng)
- [Demo](#-demo)
- [CÃ´ng nghá»‡](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
- [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [API Reference](#-api-reference)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

## ğŸ¯ Giá»›i thiá»‡u

Dá»± Ã¡n phÃ¢n tÃ­ch thá»‹ trÆ°á»ng tuyá»ƒn dá»¥ng IT táº¡i Viá»‡t Nam, thu tháº­p dá»¯ liá»‡u tá»« ITViec.com vÃ  cung cáº¥p insights chi tiáº¿t vá»:
- ğŸ’° Má»©c lÆ°Æ¡ng theo vá»‹ trÃ­, kinh nghiá»‡m, ká»¹ nÄƒng
- ğŸ› ï¸ CÃ´ng nghá»‡ vÃ  ká»¹ nÄƒng hot nháº¥t
- ğŸ¢ Top cÃ´ng ty tuyá»ƒn dá»¥ng nhiá»u
- ğŸ“ PhÃ¢n bá»‘ viá»‡c lÃ m theo thÃ nh phá»‘

### âœ¨ Äiá»ƒm Ä‘áº·c biá»‡t

ğŸ¤– **AI-powered parsing**: Sá»­ dá»¥ng Groq API (LLM Llama 3.1) Ä‘á»ƒ parse HTML thÃ´ng minh, khÃ´ng cáº§n regex phá»©c táº¡p  
âš¡ **Fully automated**: Pipeline tá»« crawl â†’ process â†’ visualize hoÃ n toÃ n tá»± Ä‘á»™ng  
ğŸ“Š **Interactive dashboard**: 4 pages vá»›i 15+ charts, filters real-time  
ğŸ”„ **Easy to extend**: Dá»… dÃ ng thÃªm data sources má»›i (TopCV, VietnamWorks...)

## âœ¨ TÃ­nh nÄƒng

### 1. ğŸ•·ï¸ AI-Powered Web Crawler
- **Browser automation**: Playwright (headless Chrome)
- **ğŸ¤– AI parsing**: Groq API vá»›i Llama 3.1 70B
  - KhÃ´ng cáº§n viáº¿t regex phá»©c táº¡p
  - Parse HTML thÃ´ng minh dá»±a vÃ o ngá»¯ cáº£nh
  - Tá»± Ä‘á»™ng adapt khi website thay Ä‘á»•i layout
- **Retry logic**: Exponential backoff khi gáº·p lá»—i
- **Logging**: Chi tiáº¿t tá»«ng bÆ°á»›c crawl
- **Async/await**: Performance tá»‘i Æ°u

### 2. ğŸ§¹ Data Processing
- Cleaning: remove duplicates, normalize text
- Transformation: parse salary, extract skills
- Classification: auto-detect job_group (Backend/Frontend/DevOps...)
- Aggregation: statistics, grouping, filtering

### 3. ğŸ“Š Dashboard
- **Overview**: Tá»•ng quan thá»‹ trÆ°á»ng (1,447 jobs, 564 companies)
- **Salary Analysis**: Box plots, histograms, comparisons
- **Skills Analysis**: Top 10 skills, word cloud, trends
- **Company Analysis**: Top recruiters, avg salary by company
- **Filters**: Location, experience, salary range, job type

### 4. ğŸ¨ Advanced Features
- Export data to CSV/Excel
- Export charts to PNG
- Multi-criteria filtering
- Responsive design

## ğŸ¬ Demo

**Dashboard Overview:**
```
ğŸ“Š Tá»•ng sá»‘ viá»‡c: 1,447
ğŸ¢ CÃ´ng ty: 564
ğŸ’° LÆ°Æ¡ng TB: 46.2M VND
ğŸ“ ThÃ nh phá»‘: 7
```

**Top Skills:**
1. JavaScript (40%)
2. Python (35%)
3. React (30%)
4. AWS (25%)
5. Docker (22%)

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

| CÃ´ng nghá»‡ | Version | Má»¥c Ä‘Ã­ch |
|-----------|---------|----------|
| Python | 3.11+ | NgÃ´n ngá»¯ chÃ­nh |
| Playwright | 1.40 | Browser automation |
| Groq API | Latest | LLM parsing (Llama 3.1) |
| Pandas | 2.0.3 | Data processing |
| Streamlit | 1.28 | Dashboard framework |
| Plotly | 5.17 | Interactive charts |

## ğŸ“¦ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone repository

```bash
git clone https://github.com/yourusername/IT-job-analysis-VN.git
cd IT-job-analysis-VN
```

### BÆ°á»›c 2: Táº¡o virtual environment

**Windows:**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**macOS/Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### BÆ°á»›c 3: CÃ i dependencies

```bash
pip install -r requirements.txt
```

### BÆ°á»›c 4: CÃ i Playwright browsers

```bash
playwright install chromium
```

### BÆ°á»›c 5: Cáº¥u hÃ¬nh Groq API key (Bá»®T BUá»˜C cho crawler)

**âš ï¸ QUAN TRá»ŒNG**: Crawler sá»­ dá»¥ng Groq API (AI) Ä‘á»ƒ parse dá»¯ liá»‡u, PHáº¢I cÃ³ API key má»›i cháº¡y Ä‘Æ°á»£c!

**Táº¡o file `.env` trong thÆ° má»¥c gá»‘c:**

```env
GROQ_API_KEY=gsk_your_actual_key_here_xxxxxxxxxx
```

**ğŸ“ Láº¥y Groq API key MIá»„N PHÃ (2 phÃºt):**

1. **Truy cáº­p**: https://console.groq.com/
2. **ÄÄƒng kÃ½** tÃ i khoáº£n (Gmail/GitHub)
3. **VÃ o API Keys**: Click "Create API Key"
4. **Copy key** (dáº¡ng `gsk_xxxxx...`)
5. **Paste vÃ o file `.env`**

**Free tier:**
- âœ… 30 requests/phÃºt (Ä‘á»§ crawl 50-100 jobs)
- âœ… Llama 3.1 70B model
- âœ… KhÃ´ng cáº§n tháº» tÃ­n dá»¥ng

### BÆ°á»›c 6: Verify installation

```bash
python -c "import playwright, pandas, streamlit; print('âœ… OK!')"
```

## ğŸš€ Sá»­ dá»¥ng

### Quick Start (Recommended)

```bash
# Cháº¡y toÃ n bá»™ pipeline
python main.py
```

Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng:
1. âœ… Crawl dá»¯ liá»‡u tá»« ITViec
2. âœ… Xá»­ lÃ½ vÃ  lÃ m sáº¡ch data
3. âœ… Khá»Ÿi Ä‘á»™ng dashboard

### Manual Steps

#### 1ï¸âƒ£ Crawl dá»¯ liá»‡u vá»›i Groq AI API

**âš ï¸ Äáº¢M Báº¢O Ä‘Ã£ cÃ³ GROQ_API_KEY trong file `.env`**

**Crawler chÃ­nh (khuyÃªn dÃ¹ng):**
```bash
# Crawl vá»›i Groq AI (parse thÃ´ng minh)
python src/crawler/ITViec_AI_groq.py --jobs 50

# Crawl nhiá»u hÆ¡n
python src/crawler/ITViec_AI_groq.py --jobs 100
```

**Hoáº·c crawler cÆ¡ báº£n (khÃ´ng cáº§n AI):**
```bash
# KhÃ´ng dÃ¹ng AI, parse báº±ng regex (Ã­t chÃ­nh xÃ¡c hÆ¡n)
python src/crawler/ITViec_crawling.py
```

**Output:** `data/raw/ITViec_data.csv` hoáº·c `data/raw/ITViec_AI_groq.csv`

**ğŸ” CÃ¡ch hoáº¡t Ä‘á»™ng:**
1. Playwright má»Ÿ browser â†’ truy cáº­p ITViec.com
2. Scroll trang Ä‘á»ƒ load jobs (lazy loading)
3. Láº¥y HTML content
4. **Gá»­i HTML Ä‘áº¿n Groq API** â†’ LLM parse thÃ nh JSON
5. LÆ°u vÃ o CSV

**ğŸ’¡ Æ¯u Ä‘iá»ƒm AI parsing:**
- âœ… KhÃ´ng cáº§n viáº¿t regex phá»©c táº¡p
- âœ… Tá»± Ä‘á»™ng adapt khi HTML thay Ä‘á»•i
- âœ… Parse thÃ´ng minh (hiá»ƒu ngá»¯ cáº£nh)
- âœ… Accuracy cao hÆ¡n 20-30%

#### 2ï¸âƒ£ Xá»­ lÃ½ dá»¯ liá»‡u

```bash
# Full pipeline
python scripts/full_pipeline.py

# Hoáº·c tá»«ng bÆ°á»›c:
python scripts/clean_data.py
python scripts/transform_data.py
python scripts/merge_and_update.py
```

Output: `data/processed/clean_data.csv`

#### 3ï¸âƒ£ Khá»Ÿi Ä‘á»™ng Dashboard

```bash
streamlit run src/visualization/dashboard_v2.py
```

Truy cáº­p: **http://localhost:8501**

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
IT-job-analysis-VN/
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ crawler/
â”‚   â”‚   â”œâ”€â”€ ITViec_crawling.py      # Main crawler
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ“‚ analysis/
â”‚   â”‚   â”œâ”€â”€ EDA.py                   # Data analysis
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ ğŸ“‚ visualization/
â”‚       â”œâ”€â”€ dashboard_v2.py          # Dashboard chÃ­nh
â”‚       â”œâ”€â”€ animations.py
â”‚       â””â”€â”€ export_tools.py
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ clean_data.py                # LÃ m sáº¡ch data
â”‚   â”œâ”€â”€ transform_data.py            # Transform data
â”‚   â”œâ”€â”€ merge_and_update.py          # Merge + dedupe
â”‚   â””â”€â”€ full_pipeline.py             # Auto pipeline
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ ğŸ“‚ raw/                      # Dá»¯ liá»‡u thÃ´
â”‚   â”‚   â””â”€â”€ ITViec_data.csv
â”‚   â””â”€â”€ ğŸ“‚ processed/                # Dá»¯ liá»‡u sáº¡ch
â”‚       â””â”€â”€ clean_data.csv
â”œâ”€â”€ ğŸ“‚ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ crawling_test.ipynb
â”‚   â”œâ”€â”€ cleanning_data.ipynb
â”‚   â””â”€â”€ eda.ipynb
â”œâ”€â”€ ğŸ“‚ config/
â”‚   â””â”€â”€ config.py                    # Config settings
â”œâ”€â”€ ğŸ“œ main.py                       # Entry point
â”œâ”€â”€ ğŸ“œ requirements.txt
â”œâ”€â”€ ğŸ“œ .env.example
â””â”€â”€ ğŸ“œ README.md
```

## ğŸ”§ API Reference

### Crawler

```python
from src.crawler.ITViec_crawling import crawl_jobs

# Basic
jobs = crawl_jobs(max_jobs=50)

# Advanced
jobs = crawl_jobs(
    max_jobs=100,
    keywords=["Python", "Java"],
    headless=True
)
```

### Data Processing

```python
from scripts.clean_data import clean_dataframe
from scripts.transform_data import transform_data

df_clean = clean_dataframe(df_raw)
df_tâŒ Lá»—i: "GROQ_API_KEY not found"
**NguyÃªn nhÃ¢n:** ChÆ°a táº¡o file `.env` hoáº·c chÆ°a cÃ³ API key

**Giáº£i phÃ¡p:**
1. Táº¡o file `.env` trong thÆ° má»¥c gá»‘c
2. Láº¥y key táº¡i: https://console.groq.com/
3. ThÃªm vÃ o `.env`: `GROQ_API_KEY=gsk_xxx...`
4. Cháº¡y láº¡i crawler

### âŒ Lá»—i: "playwright not found"
```bash
playwright install chromium
```

### âŒ Lá»—i: "Groq API rate limit exceeded" 
**NguyÃªn nhÃ¢n:** VÆ°á»£t quÃ¡ 30 requests/phÃºt (free tier)

**Giáº£i phÃ¡p:**
- Crawl Ã­t jobs hÆ¡n (--jobs 30)
- Äá»£i 1 phÃºt rá»“i cháº¡y láº¡i
- Hoáº·c upgrade Groq plan

### âŒ Lá»—i: "Invalid API key"
**NguyÃªn nhÃ¢n:** API key sai hoáº·c háº¿t háº¡n

**Giáº£i phÃ¡p:**
1. Kiá»ƒm tra key trong `.env` cÃ³ Ä‘Ãºng format `gsk_xxx...`
2. Táº¡o key má»›i táº¡i https://console.groq.com/keys
3. Update key trong `.env`
- `company`: TÃªn cÃ´ng ty
- `location`: Äá»‹a Ä‘iá»ƒm
- `salary`: Má»©c lÆ°Æ¡ng (text)
- `experience`: Kinh nghiá»‡m yÃªu cáº§u
- `skills`: Ká»¹ nÄƒng (comma-separated)

### Processed Data
- `salary_min`, `salary_max`: LÆ°Æ¡ng min/max (numeric)
- `experience_level`: Junior/Middle/Senior
- `skills_list`: Array of skills
- `job_group`: Backend/Frontend/DevOps/Data/QA/Mobile

## ğŸ› Troubleshooting

### Lá»—i: "playwright not found"
```bash
playwright install chromium
```

### Lá»—i: "Groq API rate limit exceeded"
- Free tier: 30 requests/min
- ThÃªm delay hoáº·c upgrade plan

### Dashboard khÃ´ng load data
```bash
# Kiá»ƒm tra file tá»“n táº¡i
ls data/processed/clean_data.csv

# Cháº¡y láº¡i pipeline
python scripts/full_pipeline.py
```

### Crawler timeout
- TÄƒng timeout trong config
- Kiá»ƒm tra internet connection

## ğŸ“ˆ Performance

- **Crawl 50 jobs**: ~2-3 phÃºt
- **Crawl 100 jobs**: ~5-7 phÃºt
- **Dashboard load (1,500 rows)**: <1 giÃ¢y
- **Bottleneck**: Groq API rate limit

## ğŸ¤ Contributing

Contributions welcome!

1. Fork repo
2. Create branch: `git checkout -b feature/AmazingFeature`
3. Commit: `git commit -m 'Add feature'`
4. Push: `git push origin feature/AmazingFeature`
5. Create Pull Request

## ğŸ“ TODO

- [ ] Add more data sources (TopCV, VietnamWorks)
- [ ] ML models: salary prediction, job recommendation
- [ ] Schedule auto-crawl (Airflow)
- [ ] Deploy to cloud (Heroku/AWS)
- [ ] API endpoints (FastAPI)
- [ ] Mobile app

## ğŸ“„ License

MIT License - xem [LICENSE](LICENSE)

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com

## ğŸ™ Acknowledgments

- [ITViec.com](https://itviec.com) - Data source
- [Groq](https://groq.com) - LLM API
- [Streamlit](https://streamlit.io) - Dashboard
- [Playwright](https://playwright.dev) - Automation

---

â­ **Star this repo if you find it useful!** â­

**Made with â¤ï¸ in Vietnam**
