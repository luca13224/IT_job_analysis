<div align="center">

# ğŸ‡»ğŸ‡³ Vietnam IT Job Market Analysis

### ğŸ“Š Interactive Dashboard with AI-Powered Career Insights

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.29+-red.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Data](https://img.shields.io/badge/Jobs-1,141-orange.svg)](data_clean/clean_data.csv)

*Dashboard phÃ¢n tÃ­ch thá»‹ trÆ°á»ng tuyá»ƒn dá»¥ng IT vá»›i **10 trang tÆ°Æ¡ng tÃ¡c**, AI chatbot, vÃ  cÃ´ng cá»¥ mÃ´ phá»ng lá»™ trÃ¬nh nghá» nghiá»‡p*

[Demo](#-cháº¡y-nhanh-3-bÆ°á»›c) â€¢ [Features](#-tÃ­nh-nÄƒng-chÃ­nh) â€¢ [Installation](#-cháº¡y-nhanh-3-bÆ°á»›c) â€¢ [Documentation](#-insights-chÃ­nh)

</div>

---

## ğŸš€ Cháº¡y nhanh (3 bÆ°á»›c)

### Prerequisites
- Python 3.11 or higher
- Git

### Installation

```bash
# 1. Clone repository
git clone https://github.com/luca13224/IT_job_analysis.git
cd IT_job_analysis

# 2. Create virtual environment (khuyáº¿n nghá»‹)
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run dashboard
streamlit run src/visualization/dashboard_v2.py

# 5. Má»Ÿ browser: http://localhost:8501
#    âš ï¸ KHÃ”NG dÃ¹ng 0.0.0.0:8501 (sáº½ lá»—i ERR_ADDRESS_INVALID)
```

### ğŸ¯ Quick Test
```bash
# Kiá»ƒm tra cÃ i Ä‘áº·t thÃ nh cÃ´ng
python -c "import streamlit; import pandas; import plotly; print('âœ… All dependencies OK!')"
```

## âœ¨ TÃ­nh nÄƒng chÃ­nh

### ğŸ“Š Dashboard 10 trang
1. **ğŸ  Tá»•ng quan** - Metrics tá»•ng quan thá»‹ trÆ°á»ng
2. **ğŸ“Š PhÃ¢n tÃ­ch thá»‹ trÆ°á»ng** - PhÃ¢n bá»‘ jobs theo nhÃ³m nghá»/cáº¥p Ä‘á»™/thÃ nh phá»‘
3. **ğŸ” Gá»£i Ã½ viá»‡c lÃ m** - AI matching dá»±a trÃªn ká»¹ nÄƒng (TF-IDF + Cosine Similarity)
4. **ğŸ’° PhÃ¢n tÃ­ch lÆ°Æ¡ng** - PhÃ¢n tÃ­ch chi tiáº¿t má»©c lÆ°Æ¡ng theo vá»‹ trÃ­
5. **ğŸ“ PhÃ¢n tÃ­ch ká»¹ nÄƒng** - Top skills, skill combinations, trends
6. **ğŸ¬ Ká»‹ch báº£n Demo** - 5 pre-built scenarios cho presentation
7. **ğŸš€ MÃ´ phá»ng lá»™ trÃ¬nh** - Career path 5-10 nÄƒm vá»›i salary projection
8. **âš–ï¸ CÃ´ng cá»¥ so sÃ¡nh** - So sÃ¡nh jobs/cities/companies
9. **ğŸ“¥ Xuáº¥t bÃ¡o cÃ¡o** - Export Excel/CSV/JSON + generate reports
10. **ğŸ¤– Trá»£ lÃ½ AI** - Chatbot Q&A vá» thá»‹ trÆ°á»ng IT

### ğŸ¯ Crawlers
- **ITViec.vn** - 1,141 jobs crawled
- **TopCV.vn** - Multi-page crawler vá»›i rate limiting


## ğŸ›  Tech Stack

- **Web Crawling:** Selenium, BeautifulSoup4
- **Data:** Pandas, NumPy
- **NLP:** NLTK, spaCy, Underthesea (Vietnamese)
- **ML:** Scikit-learn (TF-IDF, Cosine Similarity)
- **Visualization:** Plotly, Streamlit
- **UI/UX:** Custom CSS vá»›i gradient theme (Purple/Blue)

---

## ğŸ”„ Data Pipeline - Logic hoáº¡t Ä‘á»™ng

### ğŸ“Š Tá»•ng quan luá»“ng dá»¯ liá»‡u

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Crawling  â”‚ --> â”‚  Processing  â”‚ --> â”‚   Analysis   â”‚ --> â”‚    Dash UI   â”‚
â”‚  (Selenium) â”‚     â”‚   (Pandas)   â”‚     â”‚   (ML/NLP)   â”‚     â”‚  (Streamlit) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     1,141 jobs         Clean data         Insights/Models      10 pages
```

---

### 1ï¸âƒ£ **Web Crawling** - Thu tháº­p dá»¯ liá»‡u

**File:** `src/crawler/ITViec_crawling.py`

**Logic hoáº¡t Ä‘á»™ng:**

```python
# BÆ°á»›c 1: Khá»Ÿi táº¡o Selenium WebDriver
driver = webdriver.Chrome(ChromeDriverManager().install())
driver.get("https://itviec.com/it-jobs")

# BÆ°á»›c 2: ÄÄƒng nháº­p thá»§ cÃ´ng (Ä‘á»ƒ bypass Cloudflare)
# User Ä‘Äƒng nháº­p â†’ Nháº¥n Enter â†’ Script báº¯t Ä‘áº§u crawl

# BÆ°á»›c 3: Láº·p qua tá»«ng trang (pagination)
for page in range(1, total_pages + 1):
    # Scroll Ä‘á»ƒ load dynamic content
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
    
    # Láº¥y danh sÃ¡ch job cards
    job_cards = driver.find_elements(By.CSS_SELECTOR, ".job-card")
    
    # BÆ°á»›c 4: Láº·p qua tá»«ng job
    for card in job_cards:
        job_url = card.find_element(By.TAG_NAME, "a").get_attribute("href")
        
        # Má»Ÿ job detail page
        driver.get(job_url)
        
        # BÆ°á»›c 5: Extract thÃ´ng tin chi tiáº¿t
        job_data = {
            "job_titles": driver.find_element(By.CLASS_NAME, "job-title").text,
            "company_names": driver.find_element(By.CLASS_NAME, "company-name").text,
            "salary": driver.find_element(By.CLASS_NAME, "salary").text,
            "level": driver.find_element(By.CLASS_NAME, "level").text,
            "city": driver.find_element(By.CLASS_NAME, "address").text,
            "skills": [skill.text for skill in driver.find_elements(By.CLASS_NAME, "skill-tag")],
            "job_description": driver.find_element(By.CLASS_NAME, "job-desc").text
        }
        
        # BÆ°á»›c 6: LÆ°u vÃ o CSV ngay láº­p tá»©c (trÃ¡nh máº¥t dá»¯ liá»‡u)
        save_to_csv(job_data, "data_raw/ITViec_data.csv")
        
        # Chá» random 1-3s Ä‘á»ƒ trÃ¡nh bá»‹ block
        time.sleep(random.uniform(1, 3))
```

**Ká»¹ thuáº­t quan trá»ng:**
- âœ… **Dynamic scroll**: Load AJAX content
- âœ… **Random delays**: TrÃ¡nh bá»‹ detect bot (1-3s má»—i request)
- âœ… **Resume crawling**: LÆ°u `current_page.txt` Ä‘á»ƒ tiáº¿p tá»¥c náº¿u crash
- âœ… **Error handling**: Try-catch cho tá»«ng element, log lá»—i
- âœ… **Incremental save**: LÆ°u tá»«ng job ngay láº­p tá»©c (khÃ´ng Ä‘á»£i háº¿t)

**Output:** `data_raw/ITViec_data.csv` (1,141 rows)

---

### 2ï¸âƒ£ **Data Processing** - LÃ m sáº¡ch & chuáº©n hÃ³a

**File:** `src/data_processing/processor.py`

**Logic hoáº¡t Ä‘á»™ng:**

```python
import pandas as pd
import re

# BÆ°á»›c 1: Load raw data
df = pd.read_csv("data_raw/ITViec_data.csv")

# BÆ°á»›c 2: Clean salary (chuyá»ƒn vá» VND sá»‘)
def clean_salary(text):
    # "Up to $2,000" â†’ 46,000,000 VND (tá»· giÃ¡ 23,000)
    # "1000 - 1500 USD" â†’ 23,000,000 VND (láº¥y trung bÃ¬nh)
    # "Negotiable" â†’ NaN
    
    if "negotiable" in text.lower():
        return None
    
    # Extract numbers
    numbers = re.findall(r'\d+', text.replace(',', ''))
    
    # Check currency
    if "$" in text or "USD" in text:
        avg = sum([int(n) for n in numbers]) / len(numbers)
        return avg * 23_000  # Convert to VND
    else:
        avg = sum([int(n) for n in numbers]) / len(numbers)
        return avg * 1_000_000  # Already in triá»‡u â†’ full number

df['salary_numeric'] = df['salary'].apply(clean_salary)

# BÆ°á»›c 3: Parse skills array
def parse_skills(text):
    # "['Python', 'Django', 'AWS']" â†’ list object
    try:
        return ast.literal_eval(text)
    except:
        return []

df['array_skills'] = df['skills'].apply(parse_skills)

# BÆ°á»›c 4: Categorize job groups
def categorize_job(title):
    title_lower = title.lower()
    
    if any(word in title_lower for word in ['backend', 'java', 'python', 'golang']):
        return 'Backend Developer'
    elif any(word in title_lower for word in ['frontend', 'react', 'vue', 'angular']):
        return 'Frontend Developer'
    elif any(word in title_lower for word in ['fullstack', 'full stack', 'full-stack']):
        return 'Fullstack Developer'
    elif any(word in title_lower for word in ['data', 'ai', 'ml', 'machine learning']):
        return 'Data / AI'
    # ... more categories
    else:
        return 'Other'

df['job_group'] = df['job_titles'].apply(categorize_job)

# BÆ°á»›c 5: Standardize locations
def clean_city(text):
    if 'há»“ chÃ­ minh' in text.lower() or 'hcm' in text.lower():
        return 'Há»“ ChÃ­ Minh'
    elif 'hÃ  ná»™i' in text.lower() or 'hanoi' in text.lower():
        return 'HÃ  Ná»™i'
    # ... more cities
    else:
        return text

df['city'] = df['city'].apply(clean_city)

# BÆ°á»›c 6: Remove duplicates
df = df.drop_duplicates(subset=['job_titles', 'company_names'])

# BÆ°á»›c 7: Save clean data
df.to_csv("data_clean/clean_data.csv", index=False, encoding='utf-8-sig')
```

**Transformations:**
- âœ… **Salary normalization**: USD â†’ VND, text â†’ number
- âœ… **Skills extraction**: String â†’ List
- âœ… **Job categorization**: Title â†’ Group (Backend/Frontend/etc)
- âœ… **Location standardization**: Various formats â†’ Consistent names
- âœ… **Deduplication**: Remove same job posted multiple times

**Output:** `data_clean/clean_data.csv` (1,141 rows cleaned)

---

### 3ï¸âƒ£ **ML Models** - AI Job Recommendations

**File:** `src/ml_models/job_recommender.py`

**Logic hoáº¡t Ä‘á»™ng (TF-IDF + Cosine Similarity):**

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# BÆ°á»›c 1: Build TF-IDF Matrix
# Chuyá»ƒn skills cá»§a má»—i job thÃ nh text
skills_texts = []
for job in jobs:
    # ['Python', 'Django', 'AWS'] â†’ "python django aws"
    skills_texts.append(' '.join(job['skills']).lower())

# Táº¡o TF-IDF vectors
vectorizer = TfidfVectorizer(max_features=200)
tfidf_matrix = vectorizer.fit_transform(skills_texts)
# Shape: (1141 jobs, 200 features)

# BÆ°á»›c 2: User input skills
user_skills = ['python', 'django', 'docker']
user_text = ' '.join(user_skills)

# BÆ°á»›c 3: Transform user skills to vector
user_vector = vectorizer.transform([user_text])
# Shape: (1, 200)

# BÆ°á»›c 4: Calculate similarity vá»›i Táº¤T Cáº¢ jobs
similarities = cosine_similarity(user_vector, tfidf_matrix)
# Shape: (1, 1141)

# BÆ°á»›c 5: Rank jobs theo similarity score
df['match_score'] = similarities[0] * 100  # Convert to percentage

# BÆ°á»›c 6: Apply filters
filtered = df[
    (df['match_score'] > 0) &  # CÃ³ Ã­t nháº¥t 1 skill match
    (df['level'] == 'mid') &   # Filter by level
    (df['city'] == 'Há»“ ChÃ­ Minh')  # Filter by city
]

# BÆ°á»›c 7: Sort vÃ  return top N
recommendations = filtered.nlargest(10, 'match_score')
```

**Giáº£i thÃ­ch TF-IDF:**
- **TF (Term Frequency)**: Skill xuáº¥t hiá»‡n bao nhiÃªu láº§n trong job
- **IDF (Inverse Document Frequency)**: Skill hiáº¿m â†’ score cao hÆ¡n
- **Cosine Similarity**: GÃ³c giá»¯a 2 vectors (0-1, 1 = giá»‘ng nháº¥t)

**VÃ­ dá»¥ matching:**
```
User: ['Python', 'Django', 'AWS']

Job A: ['Python', 'Django', 'PostgreSQL', 'Redis']
â†’ Match: Python âœ“, Django âœ“ â†’ Score: 75%

Job B: ['Java', 'Spring Boot', 'MySQL']
â†’ Match: None â†’ Score: 0%

Job C: ['Python', 'Django', 'AWS', 'Docker', 'K8s']
â†’ Match: Python âœ“, Django âœ“, AWS âœ“ â†’ Score: 92%
```

---

### 4ï¸âƒ£ **NLP Analysis** - Skill Extraction & Trends

**File:** `src/nlp/skill_analyzer.py`

**Logic hoáº¡t Ä‘á»™ng:**

```python
from collections import Counter

# BÆ°á»›c 1: Flatten all skills
all_skills = []
for job in jobs:
    all_skills.extend(job['skills'])

# BÆ°á»›c 2: Count frequency
skill_counts = Counter(all_skills)

# Top 20: [('Python', 450), ('JavaScript', 380), ...]

# BÆ°á»›c 3: Skill co-occurrence (skills Ä‘i cÃ¹ng nhau)
from itertools import combinations

cooccur = Counter()
for job in jobs:
    # Táº¡o táº¥t cáº£ pairs tá»« skills cá»§a job
    for skill1, skill2 in combinations(job['skills'], 2):
        pair = tuple(sorted([skill1, skill2]))
        cooccur[pair] += 1

# Top pairs: [('Python', 'Django'), ('React', 'TypeScript'), ...]

# BÆ°á»›c 4: Skill recommendations
def recommend_skills(current_skills):
    # Find jobs cÃ³ current_skills
    similar_jobs = [job for job in jobs 
                    if any(s in job['skills'] for s in current_skills)]
    
    # Extract other skills from those jobs
    other_skills = []
    for job in similar_jobs:
        other_skills.extend([s for s in job['skills'] 
                            if s not in current_skills])
    
    # Return top suggested skills
    return Counter(other_skills).most_common(10)
```

---

### 5ï¸âƒ£ **Visualization** - Interactive Dashboard

**File:** `src/visualization/dashboard_v2.py`

**Architecture:**

```python
import streamlit as st
import plotly.express as px

# BÆ°á»›c 1: Load data (cached)
@st.cache_data
def load_data():
    return pd.read_csv("data_clean/clean_data.csv")

df = load_data()

# BÆ°á»›c 2: Sidebar filters
job_group = st.sidebar.selectbox("Job Group", df['job_group'].unique())
level = st.sidebar.selectbox("Level", df['level'].unique())

# BÆ°á»›c 3: Filter data
filtered = df[
    (df['job_group'] == job_group) &
    (df['level'] == level)
]

# BÆ°á»›c 4: Show metrics
col1, col2, col3 = st.columns(3)
col1.metric("Total Jobs", len(filtered))
col2.metric("Avg Salary", f"{filtered['salary_numeric'].mean()/1e6:.1f}M")
col3.metric("Companies", filtered['company_names'].nunique())

# BÆ°á»›c 5: Interactive charts
fig = px.bar(filtered['city'].value_counts(), 
             title="Jobs by City")
st.plotly_chart(fig)

# BÆ°á»›c 6: Job recommendations
user_skills = st.multiselect("Your Skills", all_skills)
if user_skills:
    recommender = JobRecommender()
    recommendations = recommender.recommend_by_skills(user_skills, top_n=10)
    
    for job in recommendations:
        st.markdown(f"**{job['job_titles']}** - Match: {job['match_score']:.0f}%")
```

**10 Pages:**
1. **Overview**: Metrics + charts tá»•ng quan
2. **Market Analysis**: Job distribution, trends
3. **Recommendations**: AI matching vá»›i user skills
4. **Salary Insights**: Salary ranges, percentiles
5. **Skills Analysis**: Top skills, co-occurrence
6. **Demo Scenarios**: 5 pre-built personas
7. **Career Simulator**: 5-10 year projection
8. **Compare Tool**: Side-by-side comparison
9. **Export**: Download Excel/CSV/JSON
10. **AI Chatbot**: Q&A vá» market

---

### 6ï¸âƒ£ **Career Simulator** - Salary Projection

**File:** `src/visualization/career_simulator.py`

**Logic hoáº¡t Ä‘á»™ng:**

```python
# Input: Backend Developer, Fresher, 5 years
job_group = "Backend Developer"
current_level = "fresher"
years = 5

# Career progression: fresher â†’ junior â†’ mid â†’ senior
levels = ['fresher', 'junior', 'mid', 'senior']
current_idx = levels.index(current_level)

# Simulate progression (avg 2 years per level)
timeline = []
for year in range(years + 1):
    level_idx = min(current_idx + (year // 2), len(levels) - 1)
    level = levels[level_idx]
    
    # Get salary data for this level
    salary_data = df[
        (df['job_group'] == job_group) &
        (df['level'] == level) &
        (df['salary_numeric'].notna())
    ]
    
    avg_salary = salary_data['salary_numeric'].mean()
    min_salary = salary_data['salary_numeric'].quantile(0.25)
    max_salary = salary_data['salary_numeric'].quantile(0.75)
    
    timeline.append({
        'year': year,
        'level': level,
        'avg_salary': avg_salary,
        'range': (min_salary, max_salary)
    })

# Visualize timeline vá»›i Plotly
fig = go.Figure()
fig.add_trace(go.Scatter(
    x=[t['year'] for t in timeline],
    y=[t['avg_salary']/1e6 for t in timeline],
    mode='lines+markers',
    name='Projected Salary'
))
st.plotly_chart(fig)
```

---

### 7ï¸âƒ£ **AI Chatbot** - Q&A System

**File:** `src/visualization/chatbot.py`

**Logic hoáº¡t Ä‘á»™ng:**

```python
# BÆ°á»›c 1: Detect intent tá»« user question
question = "LÆ°Æ¡ng Backend Developer lÃ  bao nhiÃªu?"

# BÆ°á»›c 2: Keyword matching
if any(word in question for word in ['lÆ°Æ¡ng', 'salary']):
    intent = 'salary'
elif any(word in question for word in ['ká»¹ nÄƒng', 'skill']):
    intent = 'skills'
# ... more intents

# BÆ°á»›c 3: Extract entities
if 'backend' in question:
    job_group = 'Backend Developer'

# BÆ°á»›c 4: Query data
if intent == 'salary':
    salary_data = df[df['job_group'] == job_group]['salary_numeric']
    
    response = f"""
    **Backend Developer Salary:**
    - Average: {salary_data.mean()/1e6:.1f}M VND
    - Median: {salary_data.median()/1e6:.1f}M VND
    - Range: {salary_data.min()/1e6:.1f}M - {salary_data.max()/1e6:.1f}M
    """

# BÆ°á»›c 5: Display response
st.chat_message("assistant").markdown(response)
```

---

## ğŸ“ˆ Performance & Scalability

**Current Stats:**
- ğŸ“Š Dataset: 1,141 jobs
- âš¡ Dashboard load time: ~2-3s (vá»›i caching)
- ğŸš€ TF-IDF build: ~0.5s
- ğŸ’¾ Memory usage: ~50MB

**Optimization techniques:**
- `@st.cache_data`: Cache loaded data
- `@st.cache_resource`: Cache ML models
- Incremental crawling: Resume tá»« last page
- Batch processing: Process 100 jobs at a time

## ğŸ“ Cáº¥u trÃºc Project

```
IT-job-analysis-VN-main/
â”œâ”€â”€ .streamlit/                    # Streamlit configuration
â”‚   â””â”€â”€ config.toml               # Server & UI settings
â”œâ”€â”€ config/                        # Application config
â”œâ”€â”€ data_clean/                    # âœ… Processed data (ready to use)
â”‚   â””â”€â”€ clean_data.csv            # 1,141 IT jobs from ITViec
â”œâ”€â”€ data_raw/                      # Raw scraped data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler/                  # Web scraping modules
â”‚   â”‚   â”œâ”€â”€ ITViec_crawling.py   # ITViec scraper
â”‚   â”‚   â””â”€â”€ topcv_crawling.py    # TopCV scraper
â”‚   â”œâ”€â”€ ml_models/                # AI/ML models
â”‚   â”‚   â””â”€â”€ job_recommender.py   # TF-IDF + Cosine Similarity
â”‚   â””â”€â”€ visualization/            # ğŸ“Š Dashboard modules (10 pages)
â”‚       â”œâ”€â”€ dashboard_v2.py       # ğŸ  Main entry point
â”‚       â”œâ”€â”€ demo_scenarios.py     # ğŸ¬ 5 pre-built demos
â”‚       â”œâ”€â”€ career_simulator.py   # ğŸš€ Career path projection
â”‚       â”œâ”€â”€ compare_tool.py       # âš–ï¸ Job/City/Company comparison
â”‚       â”œâ”€â”€ export_tools.py       # ğŸ“¥ Excel/CSV/JSON export
â”‚       â”œâ”€â”€ chatbot.py            # ğŸ¤– AI Q&A assistant
â”‚       â””â”€â”€ animations.py         # ğŸ¨ UI animations
â”œâ”€â”€ notebooks/                     # Jupyter analysis
â”‚   â””â”€â”€ eda.ipynb                 # Exploratory Data Analysis
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ run_dashboard_v2.bat          # ğŸš€ Quick launch script (Windows)
â””â”€â”€ README.md                     # ğŸ“– This file
```

> **ğŸ’¡ Tip:** Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ sáºµn táº¡i `data_clean/clean_data.csv`. Báº¡n khÃ´ng cáº§n cháº¡y crawler Ä‘á»ƒ demo!


## ğŸ“Š Insights chÃ­nh

**Thá»‘ng kÃª tá»•ng quan:**
- 1,141 jobs tá»« ITViec.vn
- 15+ nhÃ³m nghá» nghiá»‡p
- LÆ°Æ¡ng trung bÃ¬nh: 20-40M VND

**Top 5 nghá» hot:**
1. Backend Developer
2. Frontend Developer  
3. Fullstack Developer
4. Data / AI
5. Mobile Developer

**Top 5 skills cáº§n thiáº¿t:**
1. JavaScript / TypeScript
2. Python
3. React / Vue
4. Docker / Kubernetes
5. AWS / Cloud

**Insights lÆ°Æ¡ng:**
- Backend Senior: 30-50M VND
- Data/AI Engineer: 35-60M VND
- Frontend Mid: 20-35M VND
- DevOps Engineer: 30-55M VND

## ğŸ¬ Demo Scenarios (cho Presentation)

Dashboard cÃ³ 5 ká»‹ch báº£n demo sáºµn:

1. **Fresh Graduate** - Sinh viÃªn má»›i ra trÆ°á»ng tÃ¬m viá»‡c
2. **Experienced Dev** - Dev 2 nÄƒm muá»‘n Ä‘á»•i viá»‡c
3. **HR Analysis** - HR phÃ¢n tÃ­ch thá»‹ trÆ°á»ng lÆ°Æ¡ng
4. **Recruiter** - NhÃ  tuyá»ƒn dá»¥ng tÃ¬m trending skills
5. **Learner** - NgÆ°á»i há»c chá»n lá»™ trÃ¬nh (Frontend/Backend/Data)

## ğŸš€ Deploy lÃªn Streamlit Cloud

```bash
# 1. Push code lÃªn GitHub
git add .
git commit -m "Deploy dashboard"
git push origin main

# 2. VÃ o https://streamlit.io/cloud
# 3. Connect GitHub repo
# 4. Main file: src/visualization/dashboard_v2.py
# 5. Deploy!
```

## ğŸ¯ Workflow Demo gá»£i Ã½ (15-20 phÃºt)

1. **Intro (2p)** â†’ Tá»•ng quan + animated metrics
2. **Market Analysis (3p)** â†’ Charts & insights
3. **Career Planning (4p)** â†’ MÃ´ phá»ng 5-year roadmap
4. **Comparison (3p)** â†’ Backend vs Frontend
5. **AI Assistant (4p)** â†’ Live Q&A vá»›i chatbot
6. **Export (2p)** â†’ Download report
7. **Q&A (2p)** â†’ Use chatbot tráº£ lá»i audience

## ğŸ’¡ Tips sá»­ dá»¥ng

**Career Simulator:**
- Input: Job group + Current level + Years (1-10)
- Output: Timeline lÆ°Æ¡ng, skills cáº§n há»c theo nÄƒm
- Best for: Láº­p káº¿ hoáº¡ch nghá» nghiá»‡p dÃ i háº¡n

**Compare Tool:**
- So sÃ¡nh 2 jobs/cities/companies side-by-side
- Visual charts + auto insights
- Best for: ÄÆ°a ra quyáº¿t Ä‘á»‹nh nghá» nghiá»‡p

**AI Chatbot:**
- Há»i vá» lÆ°Æ¡ng, skills, xu hÆ°á»›ng, lá»™ trÃ¬nh
- Quick buttons cho cÃ¢u há»i phá»• biáº¿n
- Best for: Q&A session trong demo

**Export Tools:**
- Excel cÃ³ 2 sheets: Data + Summary
- CSV/JSON cho research
- Text reports vá»›i analysis

## ğŸ› Troubleshooting

<details>
<summary><b>Dashboard khÃ´ng cháº¡y Ä‘Æ°á»£c</b></summary>

```bash
# CÃ i láº¡i dependencies
pip install --upgrade streamlit pandas plotly

# Kiá»ƒm tra Python version (cáº§n >= 3.11)
python --version
```
</details>

<details>
<summary><b>Lá»—i "ERR_ADDRESS_INVALID" khi má»Ÿ browser</b></summary>

âš ï¸ **KhÃ´ng dÃ¹ng** `http://0.0.0.0:8501`

âœ… **DÃ¹ng:** `http://localhost:8501` hoáº·c `http://127.0.0.1:8501`
</details>

<details>
<summary><b>Thiáº¿u file data</b></summary>

Dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn táº¡i `data_clean/clean_data.csv` (1,141 jobs). KhÃ´ng cáº§n cháº¡y crawler!
</details>

<details>
<summary><b>Lá»—i import module</b></summary>

```bash
# Äáº£m báº£o cháº¡y tá»« thÆ° má»¥c gá»‘c
cd IT_job_analysis

# Kiá»ƒm tra cáº¥u trÃºc thÆ° má»¥c
ls src/visualization/dashboard_v2.py
```
</details>

<details>
<summary><b>Port 8501 Ä‘Ã£ bá»‹ chiáº¿m</b></summary>

```bash
# DÃ¹ng port khÃ¡c
streamlit run src/visualization/dashboard_v2.py --server.port 8502
```
</details>

<details>
<summary><b>Lá»—i encoding khi Ä‘á»c CSV</b></summary>

```python
# ThÃªm encoding UTF-8
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```
</details>

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Plotly Documentation](https://plotly.com/python/)

## ğŸ‘¥ ÄÃ³ng GÃ³p

Contributions are welcome! Vui lÃ²ng:

1. Fork repository
2. Create feature branch: `git checkout -b feature/AmazingFeature`
3. Commit changes: `git commit -m 'Add AmazingFeature'`
4. Push to branch: `git push origin feature/AmazingFeature`
5. Open Pull Request

## ğŸ“ License

MIT License - Xem [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ™ Acknowledgments

- Dá»¯ liá»‡u tá»« [ITViec.vn](https://itviec.com) - Vietnam's #1 IT Job Site
- Built with [Streamlit](https://streamlit.io) - The fastest way to build data apps
- Icons from [Icons8](https://icons8.com) - Free icons and design resources

## ğŸ“§ Contact & Support

- ğŸ“« GitHub Issues: [Report bugs or request features](https://github.com/luca13224/IT_job_analysis/issues)
- â­ Star this repo if you find it useful!
- ğŸ´ Fork and customize for your needs

---

<div align="center">

**Made with â¤ï¸ for the Vietnamese IT Community**

[![GitHub stars](https://img.shields.io/github/stars/luca13224/IT_job_analysis?style=social)](https://github.com/luca13224/IT_job_analysis)
[![GitHub forks](https://img.shields.io/github/forks/luca13224/IT_job_analysis?style=social)](https://github.com/luca13224/IT_job_analysis/fork)

</div>