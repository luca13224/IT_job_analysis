# ğŸ“Š PhÃ¢n tÃ­ch Thá»‹ trÆ°á»ng Tuyá»ƒn dá»¥ng IT Viá»‡t Nam

> Há»‡ thá»‘ng thu tháº­p vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u tuyá»ƒn dá»¥ng IT tá»± Ä‘á»™ng vá»›i AI-powered web crawler, data processing pipeline vÃ  interactive dashboard.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Playwright](https://img.shields.io/badge/Playwright-1.40+-green.svg)](https://playwright.dev/python/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
- [TÃ­nh nÄƒng](#-tÃ­nh-nÄƒng)
- [Demo](#-demo)
- [CÃ´ng nghá»‡](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
- [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [API Reference](#-api-reference)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

## ğŸ¯ Giá»›i thiá»‡u

Dá»± Ã¡n phÃ¢n tÃ­ch thá»‹ trÆ°á»ng tuyá»ƒn dá»¥ng IT táº¡i Viá»‡t Nam, thu tháº­p dá»¯ liá»‡u tá»« ITViec.com vÃ  cung cáº¥p insights chi tiáº¿t vá»:
- ğŸ’° Má»©c lÆ°Æ¡ng theo vá»‹ trÃ­, kinh nghiá»‡m, ká»¹ nÄƒng
- ğŸ› ï¸ CÃ´ng nghá»‡ vÃ  ká»¹ nÄƒng hot nháº¥t
- ğŸ¢ Top cÃ´ng ty tuyá»ƒn dá»¥ng nhiá»u
- ğŸ“ PhÃ¢n bá»‘ viá»‡c lÃ m theo thÃ nh phá»‘

### âœ¨ Äiá»ƒm Ä‘áº·c biá»‡t

ğŸ¤– **AI-powered parsing**: Sá»­ dá»¥ng Groq API (LLM Llama 3.1) Ä‘á»ƒ parse HTML thÃ´ng minh, khÃ´ng cáº§n regex phá»©c táº¡p  
âš¡ **Fully automated**: Pipeline tá»« crawl â†’ process â†’ visualize hoÃ n toÃ n tá»± Ä‘á»™ng  
ğŸ“Š **Interactive dashboard**: 4 pages vá»›i 15+ charts, filters real-time  
ğŸ”„ **Easy to extend**: Dá»… dÃ ng thÃªm data sources má»›i (TopCV, VietnamWorks...)

## âœ¨ TÃ­nh nÄƒng

### 1. ğŸ•·ï¸ Web Crawler
- Browser automation vá»›i **Playwright** (render JavaScript)
- AI parsing vá»›i **Groq API** (Llama 3.1 70B)
- Retry logic vá»›i exponential backoff
- Detailed logging vÃ  error handling
- Support async/await cho performance

### 2. ğŸ§¹ Data Processing
- Cleaning: remove duplicates, normalize text
- Transformation: parse salary, extract skills
- Classification: auto-detect job_group (Backend/Frontend/DevOps...)
- Aggregation: statistics, grouping, filtering

### 3. ğŸ“Š Dashboard
- **Overview**: Tá»•ng quan thá»‹ trÆ°á»ng (1,447 jobs, 564 companies)
- **Salary Analysis**: Box plots, histograms, comparisons
- **Skills Analysis**: Top 10 skills, word cloud, trends
- **Company Analysis**: Top recruiters, avg salary by company
- **Filters**: Location, experience, salary range, job type

### 4. ğŸ¨ Advanced Features
- Export data to CSV/Excel
- Export charts to PNG
- Multi-criteria filtering
- Responsive design

## ğŸ¬ Demo

**Dashboard Overview:**
```
ğŸ“Š Tá»•ng sá»‘ viá»‡c: 1,447
ğŸ¢ CÃ´ng ty: 564
ğŸ’° LÆ°Æ¡ng TB: 46.2M VND
ğŸ“ ThÃ nh phá»‘: 7
```

**Top Skills:**
1. JavaScript (40%)
2. Python (35%)
3. React (30%)
4. AWS (25%)
5. Docker (22%)

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

| CÃ´ng nghá»‡ | Version | Má»¥c Ä‘Ã­ch |
|-----------|---------|----------|
| Python | 3.11+ | NgÃ´n ngá»¯ chÃ­nh |
| Playwright | 1.40 | Browser automation |
| Groq API | Latest | LLM parsing (Llama 3.1) |
| Pandas | 2.0.3 | Data processing |
| Streamlit | 1.28 | Dashboard framework |
| Plotly | 5.17 | Interactive charts |

## ğŸ“¦ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone repository

```bash
git clone https://github.com/yourusername/IT-job-analysis-VN.git
cd IT-job-analysis-VN
```

### BÆ°á»›c 2: Táº¡o virtual environment

**Windows:**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**macOS/Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### BÆ°á»›c 3: CÃ i dependencies

```bash
pip install -r requirements.txt
```

### BÆ°á»›c 4: CÃ i Playwright browsers

```bash
playwright install chromium
```

### BÆ°á»›c 5: Cáº¥u hÃ¬nh API key

Táº¡o file `.env`:

```env
GROQ_API_KEY=your_api_key_here
```

**Láº¥y Groq API key miá»…n phÃ­:**
1. Truy cáº­p: https://console.groq.com/
2. ÄÄƒng kÃ½ tÃ i khoáº£n (free: 30 requests/min)
3. Táº¡o API key
4. Copy vÃ o `.env`

### BÆ°á»›c 6: Verify installation

```bash
python -c "import playwright, pandas, streamlit; print('âœ… OK!')"
```

## ğŸš€ Sá»­ dá»¥ng

### Quick Start (Recommended)

```bash
# Cháº¡y toÃ n bá»™ pipeline
python main.py
```

Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng:
1. âœ… Crawl dá»¯ liá»‡u tá»« ITViec
2. âœ… Xá»­ lÃ½ vÃ  lÃ m sáº¡ch data
3. âœ… Khá»Ÿi Ä‘á»™ng dashboard

### Manual Steps

#### 1ï¸âƒ£ Crawl dá»¯ liá»‡u

```bash
# Crawl 50 jobs (default)
python src/crawler/ITViec_crawling.py

# Crawl custom amount
python src/crawler/ITViec_crawling.py --jobs 100
```

Output: `data/raw/ITViec_data.csv`

#### 2ï¸âƒ£ Xá»­ lÃ½ dá»¯ liá»‡u

```bash
# Full pipeline
python scripts/full_pipeline.py

# Hoáº·c tá»«ng bÆ°á»›c:
python scripts/clean_data.py
python scripts/transform_data.py
python scripts/merge_and_update.py
```

Output: `data/processed/clean_data.csv`

#### 3ï¸âƒ£ Khá»Ÿi Ä‘á»™ng Dashboard

```bash
streamlit run src/visualization/dashboard_v2.py
```

Truy cáº­p: **http://localhost:8501**

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
IT-job-analysis-VN/
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ crawler/
â”‚   â”‚   â”œâ”€â”€ ITViec_crawling.py      # Main crawler
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ“‚ analysis/
â”‚   â”‚   â”œâ”€â”€ EDA.py                   # Data analysis
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ ğŸ“‚ visualization/
â”‚       â”œâ”€â”€ dashboard_v2.py          # Dashboard chÃ­nh
â”‚       â”œâ”€â”€ animations.py
â”‚       â””â”€â”€ export_tools.py
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ clean_data.py                # LÃ m sáº¡ch data
â”‚   â”œâ”€â”€ transform_data.py            # Transform data
â”‚   â”œâ”€â”€ merge_and_update.py          # Merge + dedupe
â”‚   â””â”€â”€ full_pipeline.py             # Auto pipeline
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ ğŸ“‚ raw/                      # Dá»¯ liá»‡u thÃ´
â”‚   â”‚   â””â”€â”€ ITViec_data.csv
â”‚   â””â”€â”€ ğŸ“‚ processed/                # Dá»¯ liá»‡u sáº¡ch
â”‚       â””â”€â”€ clean_data.csv
â”œâ”€â”€ ğŸ“‚ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ crawling_test.ipynb
â”‚   â”œâ”€â”€ cleanning_data.ipynb
â”‚   â””â”€â”€ eda.ipynb
â”œâ”€â”€ ğŸ“‚ config/
â”‚   â””â”€â”€ config.py                    # Config settings
â”œâ”€â”€ ğŸ“œ main.py                       # Entry point
â”œâ”€â”€ ğŸ“œ requirements.txt
â”œâ”€â”€ ğŸ“œ .env.example
â””â”€â”€ ğŸ“œ README.md
```

## ğŸ”§ API Reference

### Crawler

```python
from src.crawler.ITViec_crawling import crawl_jobs

# Basic
jobs = crawl_jobs(max_jobs=50)

# Advanced
jobs = crawl_jobs(
    max_jobs=100,
    keywords=["Python", "Java"],
    headless=True
)
```

### Data Processing

```python
from scripts.clean_data import clean_dataframe
from scripts.transform_data import transform_data

df_clean = clean_dataframe(df_raw)
df_transformed = transform_data(df_clean)
```

## ğŸ“Š Data Schema

### Raw Data
- `job_id`: Unique identifier
- `job_title`: TÃªn vá»‹ trÃ­
- `company`: TÃªn cÃ´ng ty
- `location`: Äá»‹a Ä‘iá»ƒm
- `salary`: Má»©c lÆ°Æ¡ng (text)
- `experience`: Kinh nghiá»‡m yÃªu cáº§u
- `skills`: Ká»¹ nÄƒng (comma-separated)

### Processed Data
- `salary_min`, `salary_max`: LÆ°Æ¡ng min/max (numeric)
- `experience_level`: Junior/Middle/Senior
- `skills_list`: Array of skills
- `job_group`: Backend/Frontend/DevOps/Data/QA/Mobile

## ğŸ› Troubleshooting

### Lá»—i: "playwright not found"
```bash
playwright install chromium
```

### Lá»—i: "Groq API rate limit exceeded"
- Free tier: 30 requests/min
- ThÃªm delay hoáº·c upgrade plan

### Dashboard khÃ´ng load data
```bash
# Kiá»ƒm tra file tá»“n táº¡i
ls data/processed/clean_data.csv

# Cháº¡y láº¡i pipeline
python scripts/full_pipeline.py
```

### Crawler timeout
- TÄƒng timeout trong config
- Kiá»ƒm tra internet connection

## ğŸ“ˆ Performance

- **Crawl 50 jobs**: ~2-3 phÃºt
- **Crawl 100 jobs**: ~5-7 phÃºt
- **Dashboard load (1,500 rows)**: <1 giÃ¢y
- **Bottleneck**: Groq API rate limit

## ğŸ¤ Contributing

Contributions welcome!

1. Fork repo
2. Create branch: `git checkout -b feature/AmazingFeature`
3. Commit: `git commit -m 'Add feature'`
4. Push: `git push origin feature/AmazingFeature`
5. Create Pull Request

## ğŸ“ TODO

- [ ] Add more data sources (TopCV, VietnamWorks)
- [ ] ML models: salary prediction, job recommendation
- [ ] Schedule auto-crawl (Airflow)
- [ ] Deploy to cloud (Heroku/AWS)
- [ ] API endpoints (FastAPI)
- [ ] Mobile app

## ğŸ“„ License

MIT License - xem [LICENSE](LICENSE)

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com

## ğŸ™ Acknowledgments

- [ITViec.com](https://itviec.com) - Data source
- [Groq](https://groq.com) - LLM API
- [Streamlit](https://streamlit.io) - Dashboard
- [Playwright](https://playwright.dev) - Automation

---

â­ **Star this repo if you find it useful!** â­

**Made with â¤ï¸ in Vietnam**
