"""
ğŸš€ AI Crawler VietnamWorks vá»›i Groq API - Dá»„ HÆ N ITVIEC!
=========================================================
VietnamWorks HTML Ä‘Æ¡n giáº£n hÆ¡n, dá»… parse hÆ¡n ITViec

âš ï¸ YÃŠU Cáº¦U:
    - Groq API key (FREE): https://console.groq.com
    - pip install groq playwright

ğŸš€ USAGE:
    python src/crawler/VietnamWorks_AI_groq.py --jobs 20

ğŸ’° CHI PHÃ: MIá»„N PHÃ (free tier: 30 req/min)
â±ï¸ THá»œI GIAN: ~1-2 phÃºt (nhanh!)
"""

import os
import sys
import io
import json
import asyncio
from pathlib import Path
from datetime import datetime
import logging
import pandas as pd
import re
from dotenv import load_dotenv

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

load_dotenv()


async def crawl_vietnamworks(num_jobs=20):
    """Crawl VietnamWorks báº±ng Playwright + Groq API"""
    
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        logger.error("âŒ ChÆ°a cÃ³ Groq API key!")
        logger.info("\nğŸ“ Láº¥y API key miá»…n phÃ­:")
        logger.info("   1. VÃ o: https://console.groq.com")
        logger.info("   2. Sign up (miá»…n phÃ­)")
        logger.info("   3. Táº¡o API key")
        logger.info("   4. ThÃªm vÃ o .env: GROQ_API_KEY=gsk_...")
        return []
    
    logger.info(f"âœ… API key loaded")
    logger.info(f"ğŸš€ Model: Llama 3.3 70B (qua Groq - cá»±c nhanh!)\n")
    
    try:
        from playwright.async_api import async_playwright
        from groq import Groq
        
        client = Groq(api_key=api_key)
        
        jobs = []  # Initialize early
        
        logger.info("ğŸŒ Äang khá»Ÿi Ä‘á»™ng browser...")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            logger.info("ğŸ“¡ Äang vÃ o VietnamWorks.com...")
            await page.goto("https://www.vietnamworks.com/it-software-jobs-i35-en", wait_until='domcontentloaded', timeout=30000)
            
            logger.info("â³ Äá»£i jobs load (3s)...")
            await page.wait_for_timeout(3000)
            
            # Scroll to load more vá»›i progress
            logger.info("ğŸ“œ Äang scroll Ä‘á»ƒ load jobs...")
            for i in range(3):
                await page.evaluate(f"window.scrollTo(0, {(i+1) * 800})")
                logger.info(f"   Scroll {i+1}/3...")
                await page.wait_for_timeout(800)
            
            logger.info("ğŸ“¸ Äang láº¥y HTML content...")
            content = await page.content()
            
            logger.info(f"ğŸ“ HTML length: {len(content):,} characters")
            
            # Extract HTML first before closing
            html_snippet = content[:25000]
            
            # Close browser early to avoid issues
            try:
                await page.close()
                await context.close()
                await browser.close()
                logger.info("âœ… Browser closed")
            except:
                pass  # Ignore close errors
            
            logger.info("ğŸ§  Äang gá»­i HTML cho Groq AI...")
            logger.info("â±ï¸ Äá»£i ~20 giÃ¢y...\n")
            
            prompt = f"""You are an expert web scraper. Extract EXACTLY {num_jobs} IT jobs from this VietnamWorks HTML.

FIND job cards/listings that contain:
- Job titles: Backend, Frontend, DevOps, Data, QA, etc
- Company names: Look for company/employer fields
- Salary: Numbers with $ or VND, or "Negotiable"/"Thoáº£ thuáº­n"
- Location: Cities like "Ho Chi Minh", "Ha Noi", "Da Nang"
- Skills: Technologies like Python, Java, React, AWS

EXTRACT these fields for EACH job:
- job_title: The position name
- company_name: Hiring company
- salary: Salary info or "Negotiable" if not shown
- level: Try to determine from title (fresher/junior/mid/senior) or use "mid"
- city: Work location (Ho Chi Minh/Ha Noi/Da Nang etc)
- skills: List of technologies (Python, Java, React, etc)
- description: Brief job summary if available

Return valid JSON array with {num_jobs} objects:
[
  {{
    "job_title": "Backend Developer",
    "company_name": "FPT Software",
    "salary": "$800-1500",
    "level": "mid",
    "city": "Ho Chi Minh",
    "skills": "Python, Django, MySQL",
    "description": "Develop backend APIs"
  }},
  ...
]

HTML content:
{html_snippet}

IMPORTANT: Return ONLY the JSON array, no other text or explanation."""
            
            # Call Groq API
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert at extracting structured job data from HTML. Return only valid JSON arrays."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,
                max_tokens=4000
            )
            
            result = response.choices[0].message.content
            logger.info("âœ… Groq Ä‘Ã£ tráº£ vá» káº¿t quáº£!")
            logger.info(f"ğŸ“ Response length: {len(result)} chars")
            
            # Parse JSON
            try:
                # Find JSON array in response
                json_start = result.find('[')
                json_end = result.rfind(']') + 1
                
                if json_start != -1 and json_end > json_start:
                    json_str = result[json_start:json_end]
                    jobs = json.loads(json_str)
                    
                    # Add metadata
                    for job in jobs:
                        job['crawled_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        job['method'] = "Playwright + Groq Llama 3.3"
                        job['source'] = "VietnamWorks"
                    
                    logger.info(f"ğŸ“Š ÄÃ£ extract {len(jobs)} jobs tá»« VietnamWorks!")
                    
                    # Show preview
                    if len(jobs) > 0:
                        logger.info(f"\nğŸ“‹ SAMPLE JOB:")
                        logger.info(f"  â€¢ {jobs[0].get('job_title', 'N/A')}")
                        logger.info(f"  â€¢ Company: {jobs[0].get('company_name', 'N/A')}")
                        logger.info(f"  â€¢ Salary: {jobs[0].get('salary', 'N/A')}")
                else:
                    logger.error("âŒ KhÃ´ng tÃ¬m tháº¥y JSON array trong response")
                    logger.info(f"\nğŸ“ Raw response:\n{result[:500]}...")
                    
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Lá»—i parse JSON: {e}")
                logger.info(f"\nğŸ“ Response:\n{result[:500]}...")
        
        return jobs  # Return outside async context
            
    except ImportError as e:
        logger.error(f"\nâŒ Thiáº¿u thÆ° viá»‡n: {e}")
        logger.info("\nğŸ“¦ CÃ i Ä‘áº·t:")
        logger.info("   pip install groq playwright")
        logger.info("   playwright install chromium")
        return []
        
    except Exception as e:
        logger.error(f"\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
        return []


def save_and_merge(jobs_data):
    """Save vÃ  merge vÃ o data chÃ­nh"""
    if len(jobs_data) == 0:
        logger.warning("âš ï¸ KhÃ´ng cÃ³ data Ä‘á»ƒ save")
        return None
    
    df = pd.DataFrame(jobs_data)
    
    # Save raw data
    output_path = Path(__file__).parent.parent.parent / "data_raw" / "VietnamWorks_AI_groq.csv"
    output_path.parent.mkdir(exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    logger.info(f"\nğŸ’¾ ÄÃ£ lÆ°u raw data: {output_path}")
    
    # Transform and merge
    try:
        main_file = Path(__file__).parent.parent.parent / "data_clean" / "clean_data.csv"
        
        if not main_file.exists():
            logger.warning("âš ï¸ ChÆ°a cÃ³ clean_data.csv, skip merge")
            return df
        
        logger.info("\nğŸ”„ Äang transform vÃ  merge...")
        
        # Transform to match schema
        df_processed = pd.DataFrame()
        df_processed['job_names'] = df['job_title']
        df_processed['company_names'] = df['company_name']
        df_processed['salaries'] = df['salary']
        df_processed['position_names'] = df['job_title']
        df_processed['kind_jobs'] = 'At office'
        df_processed['array_skills'] = df['skills']
        df_processed['locate_names'] = df['city']
        df_processed['exp_skills'] = df.get('description', 'N/A')
        df_processed['domain_arr'] = '[]'
        df_processed['post_dates_formatted'] = df['crawled_at']
        
        # Extract salary numeric
        def extract_sal(s):
            if pd.isna(s) or 'Negotiable' in str(s) or 'Thoáº£' in str(s):
                return None
            nums = re.findall(r'(\d+)', str(s))
            return sum([int(n) for n in nums]) / len(nums) * 1_000_000 if nums else None
        
        df_processed['salary_numeric'] = df['salary'].apply(extract_sal)
        
        # Standardize cities
        city_map = {
            'Há»“ ChÃ­ Minh': 'Ho Chi Minh',
            'HCM': 'Ho Chi Minh',
            'HÃ  Ná»™i': 'Ha Noi',
            'Hanoi': 'Ha Noi',
            'ÄÃ  Náºµng': 'Da Nang',
            'Danang': 'Da Nang'
        }
        df_processed['city'] = df['city'].replace(city_map)
        df_processed['level'] = df.get('level', 'mid')
        df_processed['job_group'] = df['job_title'].str.split().str[0]
        
        # Merge with existing data
        df_main = pd.read_csv(main_file)
        before = len(df_main)
        
        df_merged = pd.concat([df_main, df_processed], ignore_index=True)
        df_merged = df_merged.drop_duplicates(subset=['job_names', 'company_names'], keep='last')
        
        df_merged.to_csv(main_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"  âœ“ TrÆ°á»›c merge: {before} jobs")
        logger.info(f"  âœ“ Sau merge: {len(df_merged)} jobs")
        logger.info(f"  âœ“ ThÃªm má»›i: +{len(df_merged) - before} jobs")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i merge: {e}")
        import traceback
        traceback.print_exc()
    
    return df


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--jobs', type=int, default=20, help='Sá»‘ lÆ°á»£ng jobs cáº§n crawl')
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸš€ AI CRAWLER VIETNAMWORKS vá»›i GROQ - Dá»„ HÆ N ITVIEC!")
    print("="*70)
    print(f"ğŸ¯ Target: {args.jobs} jobs tá»« VietnamWorks.com")
    print(f"ğŸ¤– AI: Llama 3.3 70B qua Groq (miá»…n phÃ­ + nhanh)")
    print(f"ğŸ’° Chi phÃ­: $0 (MIá»„N PHÃ)")
    print(f"â±ï¸ Thá»i gian: ~1-2 phÃºt")
    print("="*70 + "\n")
    
    # Crawl
    jobs = await crawl_vietnamworks(num_jobs=args.jobs)
    
    if len(jobs) == 0:
        logger.error("\nâŒ Crawl tháº¥t báº¡i hoáº·c khÃ´ng tÃ¬m tháº¥y jobs")
        logger.info("\nğŸ’¡ Troubleshooting:")
        logger.info("   1. Check API key: https://console.groq.com")
        logger.info("   2. Check internet connection")
        logger.info("   3. VietnamWorks cÃ³ thá»ƒ thay Ä‘á»•i layout")
        return
    
    # Save
    df = save_and_merge(jobs)
    
    if df is not None:
        print("\n" + "="*70)
        print("ğŸ“Š Káº¾T QUáº¢ CRAWL VIETNAMWORKS")
        print("="*70)
        print(f"\nâœ… Crawled: {len(df)} jobs THáº¬T tá»« VietnamWorks")
        print(f"ğŸš€ API: Groq (miá»…n phÃ­)")
        print(f"ğŸ¢ Companies: {df['company_name'].nunique()}")
        print(f"ğŸŒ† Cities: {', '.join(df['city'].unique()[:5])}")
        
        print(f"\nğŸ“‹ TOP 5 JOBS:")
        cols = ['job_title', 'company_name', 'city', 'salary']
        print(df[cols].head().to_string(index=False))
        
        print("\n" + "="*70)
        print("âœ… HOÃ€N THÃ€NH - CRAWL VIETNAMWORKS THÃ€NH CÃ”NG!")
        print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
