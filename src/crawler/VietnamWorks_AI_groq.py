"""
üöÄ AI Crawler VietnamWorks v·ªõi Groq API - D·ªÑ H∆†N ITVIEC!
=========================================================
VietnamWorks HTML ƒë∆°n gi·∫£n h∆°n, d·ªÖ parse h∆°n ITViec

‚ö†Ô∏è Y√äU C·∫¶U:
    - Groq API key (FREE): https://console.groq.com
    - pip install groq playwright

üöÄ USAGE:
    python src/crawler/VietnamWorks_AI_groq.py --jobs 20

üí∞ CHI PH√ç: MI·ªÑN PH√ç (free tier: 30 req/min)
‚è±Ô∏è TH·ªúI GIAN: ~1-2 ph√∫t (nhanh!)
"""

import os
import sys
import io
import json
import asyncio
from pathlib import Path
from datetime import datetime
import logging
import pandas as pd
import re
from dotenv import load_dotenv

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

load_dotenv()


async def crawl_vietnamworks(num_jobs=20):
    """Crawl VietnamWorks b·∫±ng Playwright + Groq API"""
    
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        logger.error("‚ùå Ch∆∞a c√≥ Groq API key!")
        logger.info("\nüìù L·∫•y API key mi·ªÖn ph√≠:")
        logger.info("   1. V√†o: https://console.groq.com")
        logger.info("   2. Sign up (mi·ªÖn ph√≠)")
        logger.info("   3. T·∫°o API key")
        logger.info("   4. Th√™m v√†o .env: GROQ_API_KEY=gsk_...")
        return []
    
    logger.info(f"‚úÖ API key loaded")
    logger.info(f"üöÄ Model: Llama 3.3 70B (qua Groq - c·ª±c nhanh!)\n")
    
    try:
        from playwright.async_api import async_playwright
        from groq import Groq
        
        client = Groq(api_key=api_key)
        
        jobs = []  # Initialize early
        
        logger.info("üåê ƒêang kh·ªüi ƒë·ªông browser...")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            logger.info("üì° ƒêang v√†o VietnamWorks.com...")
            await page.goto("https://www.vietnamworks.com/it-software-jobs-i35-en", wait_until='domcontentloaded')
            
            logger.info("‚è≥ ƒê·ª£i jobs load (5s)...")
            await page.wait_for_timeout(5000)
            
            # Scroll to load more
            logger.info("üìú ƒêang scroll ƒë·ªÉ load jobs...")
            for i in range(3):
                await page.evaluate(f"window.scrollTo(0, {(i+1) * 800})")
                await page.wait_for_timeout(1000)
            
            logger.info("üì∏ ƒêang l·∫•y HTML content...")
            content = await page.content()
            
            logger.info(f"üìè HTML length: {len(content):,} characters")
            
            # Extract HTML first before closing
            html_snippet = content[:25000]
            
            # Close browser early to avoid issues
            try:
                await page.close()
                await context.close()
                await browser.close()
                logger.info("‚úÖ Browser closed")
            except:
                pass  # Ignore close errors
            
            logger.info("üß† ƒêang g·ª≠i HTML cho Groq AI...")
            logger.info("‚è±Ô∏è ƒê·ª£i ~20 gi√¢y...\n")
            
            prompt = f"""Extract {num_jobs} IT jobs from this VietnamWorks HTML.

Look for job cards/listings with:
- Job titles (usually in <h2>, <h3>, or class with "job-title")
- Company names (class with "company")
- Salary/wage information
- Location/city
- Tech skills, programming languages

For EACH job found, extract these fields:
- job_title: The position name
- company_name: Hiring company
- salary: Salary info or "Negotiable" if not shown
- level: Try to determine from title (fresher/junior/mid/senior) or use "mid"
- city: Work location (Ho Chi Minh/Ha Noi/Da Nang etc)
- skills: List of technologies (Python, Java, React, etc)
- description: Brief job summary if available

Return valid JSON array with {num_jobs} objects:
[
  {{
    "job_title": "Backend Developer",
    "company_name": "FPT Software",
    "salary": "$800-1500",
    "level": "mid",
    "city": "Ho Chi Minh",
    "skills": "Python, Django, MySQL",
    "description": "Develop backend APIs"
  }},
  ...
]

HTML content:
{html_snippet}

IMPORTANT: Return ONLY the JSON array, no other text or explanation."""
            
            # Call Groq API
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert at extracting structured job data from HTML. Return only valid JSON arrays."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,
                max_tokens=4000
            )
            
            result = response.choices[0].message.content
            logger.info("‚úÖ Groq ƒë√£ tr·∫£ v·ªÅ k·∫øt qu·∫£!")
            logger.info(f"üìù Response length: {len(result)} chars")
            
            # Parse JSON
            try:
                # Find JSON array in response
                json_start = result.find('[')
                json_end = result.rfind(']') + 1
                
                if json_start != -1 and json_end > json_start:
                    json_str = result[json_start:json_end]
                    jobs = json.loads(json_str)
                    
                    # Add metadata
                    for job in jobs:
                        job['crawled_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        job['method'] = "Playwright + Groq Llama 3.3"
                        job['source'] = "VietnamWorks"
                    
                    logger.info(f"üìä ƒê√£ extract {len(jobs)} jobs t·ª´ VietnamWorks!")
                    
                    # Show preview
                    if len(jobs) > 0:
                        logger.info(f"\nüìã SAMPLE JOB:")
                        logger.info(f"  ‚Ä¢ {jobs[0].get('job_title', 'N/A')}")
                        logger.info(f"  ‚Ä¢ Company: {jobs[0].get('company_name', 'N/A')}")
                        logger.info(f"  ‚Ä¢ Salary: {jobs[0].get('salary', 'N/A')}")
                else:
                    logger.error("‚ùå Kh√¥ng t√¨m th·∫•y JSON array trong response")
                    logger.info(f"\nüìù Raw response:\n{result[:500]}...")
                    
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå L·ªói parse JSON: {e}")
                logger.info(f"\nüìù Response:\n{result[:500]}...")
        
        return jobs  # Return outside async context
            
    except ImportError as e:
        logger.error(f"\n‚ùå Thi·∫øu th∆∞ vi·ªán: {e}")
        logger.info("\nüì¶ C√†i ƒë·∫∑t:")
        logger.info("   pip install groq playwright")
        logger.info("   playwright install chromium")
        return []
        
    except Exception as e:
        logger.error(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return []


def save_and_merge(jobs_data):
    """Save v√† merge v√†o data ch√≠nh"""
    if len(jobs_data) == 0:
        logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ data ƒë·ªÉ save")
        return None
    
    df = pd.DataFrame(jobs_data)
    
    # Save raw data
    output_path = Path(__file__).parent.parent.parent / "data_raw" / "VietnamWorks_AI_groq.csv"
    output_path.parent.mkdir(exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    logger.info(f"\nüíæ ƒê√£ l∆∞u raw data: {output_path}")
    
    # Transform and merge
    try:
        main_file = Path(__file__).parent.parent.parent / "data_clean" / "clean_data.csv"
        
        if not main_file.exists():
            logger.warning("‚ö†Ô∏è Ch∆∞a c√≥ clean_data.csv, skip merge")
            return df
        
        logger.info("\nüîÑ ƒêang transform v√† merge...")
        
        # Transform to match schema
        df_processed = pd.DataFrame()
        df_processed['job_names'] = df['job_title']
        df_processed['company_names'] = df['company_name']
        df_processed['salaries'] = df['salary']
        df_processed['position_names'] = df['job_title']
        df_processed['kind_jobs'] = 'At office'
        df_processed['array_skills'] = df['skills']
        df_processed['locate_names'] = df['city']
        df_processed['exp_skills'] = df.get('description', 'N/A')
        df_processed['domain_arr'] = '[]'
        df_processed['post_dates_formatted'] = df['crawled_at']
        
        # Extract salary numeric
        def extract_sal(s):
            if pd.isna(s) or 'Negotiable' in str(s) or 'Tho·∫£' in str(s):
                return None
            nums = re.findall(r'(\d+)', str(s))
            return sum([int(n) for n in nums]) / len(nums) * 1_000_000 if nums else None
        
        df_processed['salary_numeric'] = df['salary'].apply(extract_sal)
        
        # Standardize cities
        city_map = {
            'H·ªì Ch√≠ Minh': 'Ho Chi Minh',
            'HCM': 'Ho Chi Minh',
            'H√† N·ªôi': 'Ha Noi',
            'Hanoi': 'Ha Noi',
            'ƒê√† N·∫µng': 'Da Nang',
            'Danang': 'Da Nang'
        }
        df_processed['city'] = df['city'].replace(city_map)
        df_processed['level'] = df.get('level', 'mid')
        df_processed['job_group'] = df['job_title'].str.split().str[0]
        
        # Merge with existing data
        df_main = pd.read_csv(main_file)
        before = len(df_main)
        
        df_merged = pd.concat([df_main, df_processed], ignore_index=True)
        df_merged = df_merged.drop_duplicates(subset=['job_names', 'company_names'], keep='last')
        
        df_merged.to_csv(main_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"  ‚úì Tr∆∞·ªõc merge: {before} jobs")
        logger.info(f"  ‚úì Sau merge: {len(df_merged)} jobs")
        logger.info(f"  ‚úì Th√™m m·ªõi: +{len(df_merged) - before} jobs")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói merge: {e}")
        import traceback
        traceback.print_exc()
    
    return df


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--jobs', type=int, default=20, help='S·ªë l∆∞·ª£ng jobs c·∫ßn crawl')
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("üöÄ AI CRAWLER VIETNAMWORKS v·ªõi GROQ - D·ªÑ H∆†N ITVIEC!")
    print("="*70)
    print(f"üéØ Target: {args.jobs} jobs t·ª´ VietnamWorks.com")
    print(f"ü§ñ AI: Llama 3.3 70B qua Groq (mi·ªÖn ph√≠ + nhanh)")
    print(f"üí∞ Chi ph√≠: $0 (MI·ªÑN PH√ç)")
    print(f"‚è±Ô∏è Th·ªùi gian: ~1-2 ph√∫t")
    print("="*70 + "\n")
    
    # Crawl
    jobs = await crawl_vietnamworks(num_jobs=args.jobs)
    
    if len(jobs) == 0:
        logger.error("\n‚ùå Crawl th·∫•t b·∫°i ho·∫∑c kh√¥ng t√¨m th·∫•y jobs")
        logger.info("\nüí° Troubleshooting:")
        logger.info("   1. Check API key: https://console.groq.com")
        logger.info("   2. Check internet connection")
        logger.info("   3. VietnamWorks c√≥ th·ªÉ thay ƒë·ªïi layout")
        return
    
    # Save
    df = save_and_merge(jobs)
    
    if df is not None:
        print("\n" + "="*70)
        print("üìä K·∫æT QU·∫¢ CRAWL VIETNAMWORKS")
        print("="*70)
        print(f"\n‚úÖ Crawled: {len(df)} jobs TH·∫¨T t·ª´ VietnamWorks")
        print(f"üöÄ API: Groq (mi·ªÖn ph√≠)")
        print(f"üè¢ Companies: {df['company_name'].nunique()}")
        print(f"üåÜ Cities: {', '.join(df['city'].unique()[:5])}")
        
        print(f"\nüìã TOP 5 JOBS:")
        cols = ['job_title', 'company_name', 'city', 'salary']
        print(df[cols].head().to_string(index=False))
        
        print("\n" + "="*70)
        print("‚úÖ HO√ÄN TH√ÄNH - CRAWL VIETNAMWORKS TH√ÄNH C√îNG!")
        print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
