"""
ü§ñ AI Crawler v·ªõi Ollama (Local LLM) - MI·ªÑN PH√ç 100%
===================================================
S·ª≠ d·ª•ng Llama 3 local thay v√¨ GPT-4 - KH√îNG T·ªêN TI·ªÄN!

‚ö†Ô∏è Y√äU C·∫¶U:
    - Ollama installed (https://ollama.ai)
    - RAM: 8GB+ (16GB recommended)
    - Ch·∫°y: ollama pull llama3

üöÄ USAGE:
    python src/crawler/ITViec_AI_ollama.py --jobs 20

üí∞ CHI PH√ç: MI·ªÑN PH√ç (ch·∫°y local)
‚è±Ô∏è TH·ªúI GIAN: ~5-10 ph√∫t (ch·∫≠m h∆°n GPT)
"""

import os
import sys
import io
import json
import asyncio
from pathlib import Path
from datetime import datetime
import logging
import pandas as pd
import re

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


async def crawl_with_ollama(num_jobs=20):
    """Crawl ITViec b·∫±ng Playwright + Ollama (Local LLM)"""
    
    try:
        from playwright.async_api import async_playwright
        import ollama
        
        logger.info("üåê ƒêang kh·ªüi ƒë·ªông browser...")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            logger.info("üì° ƒêang v√†o ITViec.com...")
            await page.goto("https://itviec.com/it-jobs", wait_until="networkidle")
            await page.wait_for_timeout(3000)
            
            logger.info("üì∏ ƒêang l·∫•y HTML content...")
            content = await page.content()
            
            await browser.close()
            
            # Extract HTML snippet
            html_snippet = content[:8000]  # 8k chars
            
            logger.info("üß† ƒêang g·ª≠i HTML cho Llama 3 (local AI)...")
            logger.info("‚è±Ô∏è ƒê·ª£i ~1-2 ph√∫t (AI ƒëang ch·∫°y tr√™n m√°y b·∫°n)...\n")
            
            prompt = f"""Extract {num_jobs} jobs from this ITViec HTML.

For each job, extract:
- job_title
- company_name  
- salary (or "Negotiable")
- level (fresher/junior/mid/senior)
- city
- skills (comma-separated)
- description (brief)

Return ONLY valid JSON array:
[
  {{
    "job_title": "...",
    "company_name": "...",
    "salary": "...",
    "level": "...",
    "city": "...",
    "skills": "...",
    "description": "..."
  }}
]

HTML:
{html_snippet}

RETURN ONLY JSON, NO OTHER TEXT."""
            
            # Call Ollama (local)
            response = ollama.chat(
                model='llama3',
                messages=[
                    {
                        'role': 'system',
                        'content': 'You extract structured data from HTML. Return only valid JSON.'
                    },
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ]
            )
            
            result = response['message']['content']
            logger.info("‚úÖ Llama 3 ƒë√£ tr·∫£ v·ªÅ k·∫øt qu·∫£\n")
            
            # Parse JSON
            try:
                json_start = result.find('[')
                json_end = result.rfind(']') + 1
                
                if json_start != -1 and json_end > json_start:
                    json_str = result[json_start:json_end]
                    jobs = json.loads(json_str)
                    
                    # Add metadata
                    for job in jobs:
                        job['crawled_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        job['method'] = "Playwright + Llama3 (Local)"
                    
                    logger.info(f"üìä ƒê√£ extract {len(jobs)} jobs!")
                    return jobs
                else:
                    logger.error("‚ùå Kh√¥ng t√¨m th·∫•y JSON")
                    return []
                    
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå L·ªói parse JSON: {e}")
                logger.info(f"\nüìù Response:\n{result[:500]}...")
                return []
            
    except ImportError as e:
        logger.error(f"\n‚ùå Thi·∫øu th∆∞ vi·ªán: {e}")
        logger.info("\nüì¶ C√†i ƒë·∫∑t:")
        logger.info("   1. C√†i Ollama: https://ollama.ai/download")
        logger.info("   2. Ch·∫°y: ollama pull llama3")
        logger.info("   3. pip install ollama playwright")
        logger.info("   4. playwright install chromium")
        return []
        
    except Exception as e:
        logger.error(f"\n‚ùå L·ªói: {e}")
        logger.info("\nüí° Ki·ªÉm tra:")
        logger.info("   - Ollama ƒë√£ ch·∫°y ch∆∞a? (ollama serve)")
        logger.info("   - ƒê√£ pull model? (ollama pull llama3)")
        return []


def save_and_merge(jobs_data):
    """Save v√† merge v√†o data ch√≠nh"""
    if len(jobs_data) == 0:
        logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ data")
        return None
    
    df = pd.DataFrame(jobs_data)
    
    # Save
    output_path = Path(__file__).parent.parent.parent / "data_raw" / "ITViec_AI_ollama.csv"
    output_path.parent.mkdir(exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    logger.info(f"\nüíæ ƒê√£ l∆∞u: {output_path}")
    
    # Merge
    try:
        main_file = Path(__file__).parent.parent.parent / "data_clean" / "clean_data.csv"
        
        if not main_file.exists():
            return df
        
        logger.info("\nüîÑ ƒêang merge...")
        
        df_processed = pd.DataFrame()
        df_processed['job_names'] = df['job_title']
        df_processed['company_names'] = df['company_name']
        df_processed['salaries'] = df['salary']
        df_processed['position_names'] = df['job_title']
        df_processed['kind_jobs'] = 'At office'
        df_processed['array_skills'] = df['skills']
        df_processed['locate_names'] = df['city']
        df_processed['exp_skills'] = df['description']
        df_processed['domain_arr'] = '[]'
        df_processed['post_dates_formatted'] = df['crawled_at']
        
        def extract_sal(s):
            if pd.isna(s) or 'Negotiable' in str(s):
                return None
            nums = re.findall(r'(\d+)', str(s))
            return sum([int(n) for n in nums]) / len(nums) * 1_000_000 if nums else None
        
        df_processed['salary_numeric'] = df['salary'].apply(extract_sal)
        
        city_map = {'H·ªì Ch√≠ Minh': 'Ho Chi Minh', 'H√† N·ªôi': 'Ha Noi', 'ƒê√† N·∫µng': 'Da Nang'}
        df_processed['city'] = df['city'].replace(city_map)
        df_processed['level'] = df['level']
        df_processed['job_group'] = df['job_title'].str.split().str[0]
        
        df_main = pd.read_csv(main_file)
        before = len(df_main)
        
        df_merged = pd.concat([df_main, df_processed], ignore_index=True)
        df_merged = df_merged.drop_duplicates(subset=['job_names', 'company_names'])
        
        df_merged.to_csv(main_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"  ‚úì Tr∆∞·ªõc: {before} jobs")
        logger.info(f"  ‚úì Sau: {len(df_merged)} jobs (+{len(df_merged) - before})")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói merge: {e}")
    
    return df


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--jobs', type=int, default=20)
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ü§ñ AI CRAWLER v·ªõi OLLAMA (Local LLM) - MI·ªÑN PH√ç!")
    print("="*70)
    print(f"Target: {args.jobs} jobs t·ª´ ITViec.vn")
    print(f"AI: Llama 3 (ch·∫°y local tr√™n m√°y b·∫°n)")
    print(f"üí∞ Chi ph√≠: $0 (MI·ªÑN PH√ç)")
    print(f"‚è±Ô∏è Th·ªùi gian: ~5-10 ph√∫t")
    print("="*70 + "\n")
    
    # Check Ollama
    try:
        import ollama
        models = ollama.list()
        logger.info(f"‚úÖ Ollama ƒë√£ c√†i\n")
    except:
        logger.error("‚ùå Ch∆∞a c√†i Ollama!")
        logger.info("\nüì¶ C√†i ƒë·∫∑t:")
        logger.info("   1. T·∫£i Ollama: https://ollama.ai/download")
        logger.info("   2. C√†i ƒë·∫∑t v√† ch·∫°y")
        logger.info("   3. Ch·∫°y: ollama pull llama3")
        logger.info("   4. pip install ollama")
        return
    
    # Crawl
    jobs = await crawl_with_ollama(num_jobs=args.jobs)
    
    if len(jobs) == 0:
        logger.error("\n‚ùå Crawl th·∫•t b·∫°i")
        return
    
    # Save
    df = save_and_merge(jobs)
    
    if df is not None:
        print("\n" + "="*70)
        print("üìä K·∫æT QU·∫¢")
        print("="*70)
        print(f"\n‚úÖ Crawled: {len(df)} jobs TH·∫¨T")
        print(f"ü§ñ AI: Llama 3 (local, mi·ªÖn ph√≠)")
        print(f"üè¢ Companies: {df['company_name'].nunique()}")
        
        print(f"\nüìã SAMPLE:")
        cols = ['job_title', 'company_name', 'city']
        print(df[cols].head().to_string(index=False))
        
        print("\n" + "="*70)
        print("‚úÖ HO√ÄN TH√ÄNH - KH√îNG T·ªêN TI·ªÄN!")
        print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
