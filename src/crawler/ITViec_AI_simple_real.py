"""
ğŸ¤– AI CRAWLER THáº¬T - Version Ä‘Æ¡n giáº£n vá»›i Playwright + GPT
==========================================================
KhÃ´ng cáº§n browser-use, chá»‰ dÃ¹ng Playwright + OpenAI API trá»±c tiáº¿p

âš ï¸ YÃŠU Cáº¦U:
    - OpenAI API key
    - playwright
    
ğŸš€ USAGE:
    python src/crawler/ITViec_AI_simple_real.py --jobs 20
"""

import os
import sys
import io
import json
import asyncio
from pathlib import Path
from datetime import datetime
import logging
from dotenv import load_dotenv
import pandas as pd
import re

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

load_dotenv()


async def crawl_with_playwright_and_ai(num_jobs=20):
    """Crawl ITViec báº±ng Playwright + GPT Ä‘á»ƒ parse HTML"""
    
    api_key = os.getenv("OPENAI_API_KEY")
    model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
    
    if not api_key or api_key == "sk-your-openai-api-key-here":
        logger.error("âŒ ChÆ°a cÃ³ OpenAI API key trong file .env!")
        return []
    
    logger.info(f"âœ… API key loaded")
    logger.info(f"ğŸ¤– Model: {model}\n")
    
    try:
        from playwright.async_api import async_playwright
        from openai import OpenAI
        
        client = OpenAI(api_key=api_key)
        
        logger.info("ğŸŒ Äang khá»Ÿi Ä‘á»™ng browser...")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            logger.info("ğŸ“¡ Äang vÃ o ITViec.com...")
            await page.goto("https://itviec.com/it-jobs", wait_until="networkidle")
            await page.wait_for_timeout(3000)
            
            logger.info("ğŸ“¸ Äang láº¥y HTML content...")
            content = await page.content()
            
            # Láº¥y job cards
            job_cards = await page.query_selector_all('[class*="job"]')
            logger.info(f"âœ… TÃ¬m tháº¥y {len(job_cards)} job cards\n")
            
            await browser.close()
            
            # Extract má»™t pháº§n HTML Ä‘á»ƒ gá»­i cho GPT
            html_snippet = content[:10000]  # Láº¥y 10k chars Ä‘áº§u
            
            logger.info("ğŸ§  Äang gá»­i HTML cho GPT Ä‘á»ƒ extract data...")
            logger.info("â±ï¸ Äá»£i ~30 giÃ¢y...\n")
            
            prompt = f"""
PhÃ¢n tÃ­ch HTML sau tá»« trang ITViec.com vÃ  extract {num_jobs} jobs Ä‘áº§u tiÃªn.

Vá»›i má»—i job, extract:
- job_title: TiÃªu Ä‘á» cÃ´ng viá»‡c
- company_name: TÃªn cÃ´ng ty  
- salary: Má»©c lÆ°Æ¡ng (náº¿u khÃ´ng cÃ³ ghi "Negotiable")
- level: Cáº¥p Ä‘á»™ (fresher/junior/mid/senior)
- city: ThÃ nh phá»‘ (Há»“ ChÃ­ Minh/HÃ  Ná»™i/ÄÃ  Náºµng...)
- skills: CÃ¡c skills yÃªu cáº§u (cÃ¡ch nhau bá»Ÿi dáº¥u pháº©y)
- description: MÃ´ táº£ ngáº¯n

Tráº£ vá» JSON array:
[
  {{
    "job_title": "...",
    "company_name": "...",
    "salary": "...",
    "level": "...",
    "city": "...",
    "skills": "...",
    "description": "..."
  }}
]

HTML:
{html_snippet}

CHá»ˆ TRáº¢ Vá»€ JSON ARRAY, KHÃ”NG GHI GÃŒ THÃŠM.
"""
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an expert at extracting structured data from HTML. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1
            )
            
            result = response.choices[0].message.content
            logger.info("âœ… GPT Ä‘Ã£ tráº£ vá» káº¿t quáº£\n")
            
            # Parse JSON
            try:
                # Extract JSON tá»« response (cÃ³ thá»ƒ cÃ³ ```json wrapper)
                json_start = result.find('[')
                json_end = result.rfind(']') + 1
                
                if json_start != -1 and json_end > json_start:
                    json_str = result[json_start:json_end]
                    jobs = json.loads(json_str)
                    
                    # Add metadata
                    for job in jobs:
                        job['crawled_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        job['method'] = f"Playwright + {model}"
                    
                    logger.info(f"ğŸ“Š ÄÃ£ extract {len(jobs)} jobs tá»« HTML!")
                    return jobs
                else:
                    logger.error("âŒ KhÃ´ng tÃ¬m tháº¥y JSON trong response")
                    return []
                    
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Lá»—i parse JSON: {e}")
                logger.info(f"\nğŸ“ Response tá»« GPT:\n{result[:500]}...")
                return []
            
    except ImportError as e:
        logger.error(f"\nâŒ Thiáº¿u thÆ° viá»‡n: {e}")
        logger.info("\nğŸ“¦ CÃ i Ä‘áº·t:")
        logger.info("   pip install playwright openai")
        logger.info("   playwright install chromium")
        return []
        
    except Exception as e:
        logger.error(f"\nâŒ Lá»—i: {e}")
        return []


def save_and_merge(jobs_data):
    """Save vÃ  merge vÃ o data chÃ­nh"""
    if len(jobs_data) == 0:
        logger.warning("âš ï¸ KhÃ´ng cÃ³ data Ä‘á»ƒ lÆ°u")
        return None
    
    df = pd.DataFrame(jobs_data)
    
    # Save to data_raw
    output_path = Path(__file__).parent.parent.parent / "data_raw" / "ITViec_AI_real.csv"
    output_path.parent.mkdir(exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    logger.info(f"\nğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_path}")
    
    # Auto merge
    try:
        main_file = Path(__file__).parent.parent.parent / "data_clean" / "clean_data.csv"
        
        if not main_file.exists():
            logger.info("âš ï¸ KhÃ´ng tÃ¬m tháº¥y main data file")
            return df
        
        logger.info("\nğŸ”„ Äang merge vÃ o data chÃ­nh...")
        
        # Transform
        df_processed = pd.DataFrame()
        df_processed['job_names'] = df['job_title']
        df_processed['company_names'] = df['company_name']
        df_processed['salaries'] = df['salary']
        df_processed['position_names'] = df['job_title']
        df_processed['kind_jobs'] = 'At office'
        df_processed['array_skills'] = df['skills']
        df_processed['locate_names'] = df['city']
        df_processed['exp_skills'] = df['description']
        df_processed['domain_arr'] = '[]'
        df_processed['post_dates_formatted'] = df['crawled_at']
        
        # Extract salary numeric
        def extract_sal(s):
            if pd.isna(s) or 'Negotiable' in str(s):
                return None
            nums = re.findall(r'(\d+)', str(s))
            return sum([int(n) for n in nums]) / len(nums) * 1_000_000 if nums else None
        
        df_processed['salary_numeric'] = df['salary'].apply(extract_sal)
        
        # Normalize
        city_map = {'Há»“ ChÃ­ Minh': 'Ho Chi Minh', 'HÃ  Ná»™i': 'Ha Noi', 'ÄÃ  Náºµng': 'Da Nang'}
        df_processed['city'] = df['city'].replace(city_map)
        df_processed['level'] = df['level']
        df_processed['job_group'] = df['job_title'].str.split().str[0]
        
        # Load and merge
        df_main = pd.read_csv(main_file)
        before = len(df_main)
        
        df_merged = pd.concat([df_main, df_processed], ignore_index=True)
        df_merged = df_merged.drop_duplicates(subset=['job_names', 'company_names'])
        
        df_merged.to_csv(main_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"  âœ“ TrÆ°á»›c: {before} jobs")
        logger.info(f"  âœ“ Sau: {len(df_merged)} jobs (+{len(df_merged) - before})")
        logger.info(f"  ğŸ’¾ ÄÃ£ lÆ°u: {main_file}")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i merge: {e}")
    
    return df


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--jobs', type=int, default=20)
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸ¤– AI CRAWLER THáº¬T - Playwright + GPT")
    print("="*70)
    print(f"Target: {args.jobs} jobs tá»« ITViec.vn")
    print(f"Method: Playwright crawl HTML â†’ GPT extract data")
    print(f"ğŸ’° Chi phÃ­: ~$0.05-0.10")
    print("="*70 + "\n")
    
    # Crawl
    jobs = await crawl_with_playwright_and_ai(num_jobs=args.jobs)
    
    if len(jobs) == 0:
        logger.error("\nâŒ Crawl tháº¥t báº¡i")
        return
    
    # Save and merge
    df = save_and_merge(jobs)
    
    if df is not None:
        print("\n" + "="*70)
        print("ğŸ“Š Káº¾T QUáº¢ CRAWL")
        print("="*70)
        print(f"\nâœ… ÄÃ£ crawl: {len(df)} jobs THáº¬T tá»« ITViec.vn")
        print(f"ğŸ¢ CÃ´ng ty: {df['company_name'].nunique()}")
        print(f"ğŸ™ï¸ ThÃ nh phá»‘: {df['city'].nunique()}")
        
        print(f"\nğŸ“‹ MáºªU 5 JOBS:")
        print("="*70)
        cols = ['job_title', 'company_name', 'salary', 'city']
        print(df[cols].head().to_string(index=False))
        
        print("\n" + "="*70)
        print("âœ… HOÃ€N THÃ€NH!")
        print("ğŸ’¾ Data Ä‘Ã£ merge vÃ o: data_clean/clean_data.csv")
        print("ğŸ”„ Refresh dashboard Ä‘á»ƒ xem")
        print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
