"""
ğŸ¤– AI CRAWLER THáº¬T - Crawl data tháº­t tá»« ITViec.vn báº±ng AI
==========================================================
Sá»­ dá»¥ng GPT-4 + Browser automation Ä‘á»ƒ tá»± Ä‘á»™ng crawl

âš ï¸ YÃŠU Cáº¦U:
    - OpenAI API key (trong file .env)
    - Python 3.11+
    - browser-use, langchain-openai

ğŸš€ CÃCH DÃ™NG:
    python src/crawler/ITViec_AI_real.py --jobs 50
    python src/crawler/ITViec_AI_real.py --jobs 100
    python src/crawler/ITViec_AI_real.py --quick  # 20 jobs

ğŸ’° CHI PHÃ:
    ~$0.50 cho 100 jobs (GPT-3.5-turbo)
    ~$2.00 cho 100 jobs (GPT-4)

â±ï¸ THá»œI GIAN:
    ~10-15 phÃºt cho 100 jobs
"""

import os
import sys
import io
import asyncio
import pandas as pd
from datetime import datetime
from pathlib import Path
import logging
import re
from dotenv import load_dotenv

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()


class RealAICrawler:
    """AI crawler tháº­t sá»­ dá»¥ng GPT-4 + Browser automation"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        self.jobs_data = []
        
        if not self.api_key or self.api_key == "sk-your-openai-api-key-here":
            raise ValueError(
                "âŒ ChÆ°a cÃ³ OpenAI API key!\n"
                "ğŸ“ HÆ°á»›ng dáº«n:\n"
                "   1. Má»Ÿ file .env\n"
                "   2. Láº¥y API key tá»«: https://platform.openai.com/api-keys\n"
                "   3. Thay tháº¿ OPENAI_API_KEY=sk-your-key\n"
            )
        
        logger.info(f"âœ… ÄÃ£ load API key")
        logger.info(f"ğŸ¤– Sá»­ dá»¥ng model: {self.model}")
        
    async def setup_browser_agent(self):
        """Setup browser-use agent vá»›i GPT"""
        try:
            from browser_use import Agent
            from langchain_openai import ChatOpenAI
            
            logger.info("ğŸŒ Äang khá»Ÿi táº¡o AI browser agent...")
            
            # Initialize LLM
            llm = ChatOpenAI(
                model=self.model,
                openai_api_key=self.api_key,
                temperature=0.1  # Low temperature for consistent extraction
            )
            
            # Create browser agent
            self.agent = Agent(
                task="Navigate to ITViec.com and extract IT job listings",
                llm=llm
            )
            
            logger.info("âœ… Browser agent sáºµn sÃ ng!")
            return True
            
        except ImportError:
            logger.error("âŒ ChÆ°a cÃ i browser-use!")
            logger.info("\nğŸ“¦ CÃ i Ä‘áº·t:")
            logger.info("   pip install browser-use langchain-openai playwright")
            logger.info("   playwright install chromium")
            return False
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i setup: {e}")
            return False
    
    async def crawl_with_ai(self, num_jobs=50):
        """Sá»­ dá»¥ng AI Ä‘á»ƒ crawl tháº­t tá»« ITViec.vn"""
        logger.info(f"\nğŸ¤– AI Ä‘ang vÃ o ITViec.vn vÃ  crawl {num_jobs} jobs...")
        logger.info("â±ï¸ Thá»i gian Æ°á»›c tÃ­nh: 10-15 phÃºt")
        logger.info("ğŸ’° Chi phÃ­ Æ°á»›c tÃ­nh: $0.25-0.50\n")
        
        try:
            from browser_use import Agent
            from langchain_openai import ChatOpenAI
            
            # Create LLM
            llm = ChatOpenAI(
                model=self.model,
                openai_api_key=self.api_key,
                temperature=0.1
            )
            
            # Create agent with detailed task
            task = f"""
VÃ o trang https://itviec.com/it-jobs vÃ  crawl {num_jobs} cÃ´ng viá»‡c IT.

Vá»›i má»—i cÃ´ng viá»‡c, trÃ­ch xuáº¥t:
1. job_title (tiÃªu Ä‘á» cÃ´ng viá»‡c)
2. company_name (tÃªn cÃ´ng ty)
3. salary (má»©c lÆ°Æ¡ng, náº¿u khÃ´ng cÃ³ ghi "Negotiable")
4. level (cáº¥p Ä‘á»™: fresher, junior, mid, senior...)
5. city (thÃ nh phá»‘: Há»“ ChÃ­ Minh, HÃ  Ná»™i, ÄÃ  Náºµng...)
6. skills (cÃ¡c ká»¹ nÄƒng yÃªu cáº§u, ngÄƒn cÃ¡ch bá»Ÿi dáº¥u pháº©y)
7. description (mÃ´ táº£ ngáº¯n vá» cÃ´ng viá»‡c)

Scroll xuá»‘ng Ä‘á»ƒ load thÃªm jobs náº¿u cáº§n.
Click "Load more" hoáº·c "Xem thÃªm" náº¿u cÃ³.

Tráº£ vá» káº¿t quáº£ dÆ°á»›i dáº¡ng JSON array:
[
  {{
    "job_title": "Backend Developer",
    "company_name": "VNG Corporation",
    "salary": "30-40 triá»‡u VND",
    "level": "mid",
    "city": "Há»“ ChÃ­ Minh",
    "skills": "Python, Django, PostgreSQL, Docker",
    "description": "Develop scalable backend systems"
  }},
  ...
]
"""

            logger.info("ğŸ§  AI Ä‘ang phÃ¢n tÃ­ch trang web...")
            agent = Agent(task=task, llm=llm)
            
            # Run agent
            result = await agent.run()
            
            logger.info("âœ… AI Ä‘Ã£ hoÃ n thÃ nh crawl!")
            
            # Parse result
            self.jobs_data = self._parse_ai_result(result)
            
            if len(self.jobs_data) > 0:
                logger.info(f"ğŸ“Š ÄÃ£ crawl Ä‘Æ°á»£c {len(self.jobs_data)} jobs tháº­t tá»« web")
            else:
                logger.warning("âš ï¸ KhÃ´ng parse Ä‘Æ°á»£c data. Thá»­ láº¡i vá»›i model khÃ¡c (gpt-4)")
                
            return self.jobs_data
            
        except ImportError:
            logger.error("\nâŒ Thiáº¿u thÆ° viá»‡n! CÃ i Ä‘áº·t:")
            logger.info("   pip install browser-use langchain-openai playwright")
            logger.info("   playwright install chromium")
            return []
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i crawl: {e}")
            logger.info("\nğŸ’¡ Thá»­:")
            logger.info("   1. Check API key trong .env")
            logger.info("   2. Äá»•i model sang gpt-4 trong .env")
            logger.info("   3. Check balance: https://platform.openai.com/usage")
            return []
    
    def _parse_ai_result(self, result):
        """Parse káº¿t quáº£ tá»« AI"""
        import json
        
        # Try to extract JSON from result
        result_str = str(result)
        
        # Find JSON array
        start_idx = result_str.find('[')
        end_idx = result_str.rfind(']') + 1
        
        if start_idx != -1 and end_idx > start_idx:
            json_str = result_str[start_idx:end_idx]
            try:
                jobs = json.loads(json_str)
                
                # Add metadata
                for job in jobs:
                    job['crawled_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    job['method'] = f"AI-Powered Real ({self.model})"
                
                return jobs
            except json.JSONDecodeError:
                logger.warning("âš ï¸ KhÃ´ng parse Ä‘Æ°á»£c JSON tá»« AI response")
                return []
        
        return []
    
    def save_results(self):
        """Save crawled data"""
        if len(self.jobs_data) == 0:
            logger.warning("âš ï¸ KhÃ´ng cÃ³ data Ä‘á»ƒ lÆ°u")
            return None
        
        # Convert to DataFrame
        df = pd.DataFrame(self.jobs_data)
        
        # Save to data_raw/
        output_path = Path(__file__).parent.parent.parent / "data_raw" / "ITViec_AI_real.csv"
        output_path.parent.mkdir(exist_ok=True)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"\nğŸ’¾ ÄÃ£ lÆ°u vÃ o: {output_path}")
        return df
    
    def auto_merge_to_main(self):
        """Tá»± Ä‘á»™ng gá»™p vÃ o data_clean/clean_data.csv"""
        try:
            project_root = Path(__file__).parent.parent.parent
            ai_file = project_root / "data_raw" / "ITViec_AI_real.csv"
            main_file = project_root / "data_clean" / "clean_data.csv"
            
            if not ai_file.exists():
                logger.info("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file AI data Ä‘á»ƒ merge")
                return
            
            logger.info("\nğŸ”„ Tá»± Ä‘á»™ng gá»™p data AI vÃ o data chÃ­nh...")
            
            # Load AI data
            df_ai = pd.read_csv(ai_file)
            
            # Transform to standard format
            df_processed = pd.DataFrame()
            df_processed['job_names'] = df_ai['job_title']
            df_processed['company_names'] = df_ai['company_name']
            df_processed['salaries'] = df_ai['salary']
            df_processed['position_names'] = df_ai['job_title']
            df_processed['kind_jobs'] = 'At office'
            df_processed['array_skills'] = df_ai['skills']
            df_processed['locate_names'] = df_ai['city']
            df_processed['exp_skills'] = df_ai['description']
            df_processed['domain_arr'] = '[]'
            df_processed['post_dates_formatted'] = df_ai['crawled_at']
            
            # Extract salary_numeric
            def extract_salary_num(s):
                if pd.isna(s) or s == 'Negotiable' or 'Negotiable' in str(s):
                    return None
                nums = re.findall(r'(\d+)', str(s))
                if nums:
                    return sum([int(n) for n in nums]) / len(nums) * 1_000_000
                return None
            
            df_processed['salary_numeric'] = df_ai['salary'].apply(extract_salary_num)
            
            # Normalize cities
            city_map = {
                'Há»“ ChÃ­ Minh': 'Ho Chi Minh',
                'HÃ  Ná»™i': 'Ha Noi',
                'ÄÃ  Náºµng': 'Da Nang',
                'Cáº§n ThÆ¡': 'Can Tho',
                'Háº£i PhÃ²ng': 'Hai Phong'
            }
            df_processed['city'] = df_ai['city'].replace(city_map)
            df_processed['level'] = df_ai['level']
            df_processed['job_group'] = df_ai['job_title'].str.split(' ').str[0]
            
            # Load existing data
            if main_file.exists():
                df_main = pd.read_csv(main_file)
                logger.info(f"  âœ“ Data hiá»‡n cÃ³: {len(df_main)} jobs")
                
                # Merge
                df_merged = pd.concat([df_main, df_processed], ignore_index=True)
                df_merged = df_merged.drop_duplicates(subset=['job_names', 'company_names'], keep='first')
                
                logger.info(f"  âœ“ Tá»•ng sau gá»™p: {len(df_merged)} jobs (thÃªm {len(df_merged) - len(df_main)} jobs má»›i)")
            else:
                df_merged = df_processed
                logger.info(f"  âœ“ Táº¡o má»›i data: {len(df_merged)} jobs")
            
            # Save
            df_merged.to_csv(main_file, index=False, encoding='utf-8-sig')
            logger.info(f"  ğŸ’¾ ÄÃ£ lÆ°u: {main_file}")
            logger.info("  ğŸ¯ Dashboard sáº½ tá»± Ä‘á»™ng dÃ¹ng data má»›i!")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi merge: {e}")


async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='AI Crawler THáº¬T - Crawl tá»« ITViec.vn')
    parser.add_argument('--jobs', type=int, default=50, 
                       help='Sá»‘ lÆ°á»£ng jobs cáº§n crawl (default: 50)')
    parser.add_argument('--quick', action='store_true', 
                       help='Demo nhanh vá»›i 20 jobs')
    args = parser.parse_args()
    
    num_jobs = 20 if args.quick else args.jobs
    
    print("\n" + "="*70)
    print("ğŸ¤– AI CRAWLER THáº¬T - CRAWL DATA THá»°C Tá»ª WEB")
    print("="*70)
    print(f"Má»¥c tiÃªu: Crawl {num_jobs} jobs THáº¬T tá»« ITViec.vn")
    print(f"PhÆ°Æ¡ng thá»©c: AI tá»± vÃ o web, tá»± Ä‘iá»u hÆ°á»›ng, tá»± trÃ­ch xuáº¥t")
    print(f"â±ï¸ Thá»i gian: ~10-15 phÃºt")
    print(f"ğŸ’° Chi phÃ­: ~${0.005 * num_jobs:.2f}")
    print("="*70 + "\n")
    
    try:
        # Initialize crawler
        crawler = RealAICrawler()
        
        # Crawl with AI
        jobs = await crawler.crawl_with_ai(num_jobs=num_jobs)
        
        if len(jobs) == 0:
            logger.error("\nâŒ KhÃ´ng crawl Ä‘Æ°á»£c data")
            logger.info("\nğŸ’¡ NguyÃªn nhÃ¢n cÃ³ thá»ƒ:")
            logger.info("   1. API key háº¿t háº¡n/khÃ´ng há»£p lá»‡")
            logger.info("   2. Model khÃ´ng Ä‘á»§ máº¡nh (thá»­ gpt-4)")
            logger.info("   3. Website thay Ä‘á»•i cáº¥u trÃºc")
            logger.info("   4. Thiáº¿u thÆ° viá»‡n browser-use")
            return
        
        # Save results
        df = crawler.save_results()
        
        # Auto merge
        crawler.auto_merge_to_main()
        
        # Show statistics
        print("\n" + "="*70)
        print("ğŸ“Š THá»NG KÃŠ Dá»® LIá»†U (DATA THáº¬T Tá»ª WEB)")
        print("="*70)
        
        print(f"\nğŸ”¢ Tá»•ng quan:")
        print(f"   â€¢ Tá»•ng jobs crawled: {len(df)}")
        print(f"   â€¢ CÃ´ng ty: {df['company_name'].nunique()}")
        print(f"   â€¢ ThÃ nh phá»‘: {df['city'].nunique()}")
        
        print(f"\nğŸ™ï¸ Top 5 Cities:")
        for city, count in df['city'].value_counts().head(5).items():
            print(f"   â€¢ {city}: {count} jobs")
        
        print(f"\nğŸ¢ Top 5 Companies:")
        for company, count in df['company_name'].value_counts().head(5).items():
            print(f"   â€¢ {company}: {count} jobs")
        
        # Show sample
        print(f"\nğŸ“‹ MáºªU 5 JOBS Äáº¦U TIÃŠN (DATA THáº¬T):")
        print("="*70)
        sample_cols = ['job_title', 'company_name', 'salary', 'level', 'city']
        print(df[sample_cols].head().to_string(index=False))
        
        print("\n" + "="*70)
        print("âœ… HOÃ€N THÃ€NH CRAWL THáº¬T!")
        print("="*70)
        print(f"ğŸ“Š ÄÃ£ crawl: {len(df)} jobs THáº¬T tá»« ITViec.vn")
        print(f"ğŸ’¾ Data Ä‘Ã£ gá»™p vÃ o: data_clean/clean_data.csv")
        print(f"ğŸ”„ Refresh dashboard Ä‘á»ƒ xem data má»›i")
        print()
        
    except ValueError as e:
        logger.error(f"\n{e}")
    except Exception as e:
        logger.error(f"\nâŒ Lá»—i: {e}")
        logger.info("\nğŸ“ Debug:")
        logger.info("   1. Check .env cÃ³ OPENAI_API_KEY chÆ°a")
        logger.info("   2. CÃ i: pip install browser-use langchain-openai playwright")
        logger.info("   3. playwright install chromium")


if __name__ == "__main__":
    asyncio.run(main())
