"""
üé≠ AI Crawler Demo (Enhanced Mock Version - Nhi·ªÅu data realistic h∆°n)
===================================================================
M√¥ ph·ªèng AI crawl v·ªõi 50-200 jobs t·ª´ 50+ c√¥ng ty, 6 job types

üöÄ C√ÅCH D√ôNG:
    python src/crawler/ITViec_AI_demo_v2.py             # 100 jobs (m·∫∑c ƒë·ªãnh)
    python src/crawler/ITViec_AI_demo_v2.py --jobs 200  # 200 jobs
    python src/crawler/ITViec_AI_demo_v2.py --quick     # 10 jobs (demo nhanh)

üíæ OUTPUT:
    - data_raw/ITViec_AI_demo.csv
    - T·ª± ƒë·ªông g·ªôp v√†o data_clean/clean_data.csv
"""

import pandas as pd
from datetime import datetime
import time
import random
import logging
import re
from pathlib import Path
import sys
import io

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class AIJobCrawler:
    """AI-powered crawler v·ªõi nhi·ªÅu data realistic"""
    
    def __init__(self):
        self.jobs_data = []
        self.setup_data_sources()
        
    def setup_data_sources(self):
        """Setup danh s√°ch c√¥ng ty, skills, cities..."""
        # 50+ c√¥ng ty n·ªïi ti·∫øng VN
        self.companies = [
            "VNG Corporation", "FPT Software", "Viettel Solutions", "MOMO", "Tiki",
            "Shopee Vietnam", "Grab Vietnam", "Zalo", "VinID", "TechComBank",
            "VPBank Digital", "Be Group", "Sendo", "Teko", "The Gioi Di Dong",
            "VinBrain", "VinBigData", "Sun Asterisk", "KMS Technology", "NashTech",
            "CMC Corporation", "FPT Telecom", "Viettel Post", "GHN Express", "Ninja Van",
            "GoViet", "Foody", "Lozi", "Chotot", "5giay.vn",
            "VNLife", "VNPay", "Moca", "AirPay", "ShopeePay",
            "Lazada", "Adayroi", "Vatgia", "123job", "CareerLink",
            "Samsung Vietnam", "LG Electronics", "Bosch Vietnam", "Siemens",
            "BKAV", "Kaspersky Vietnam", "FPT IS", "Viettel Cyber Security",
            "Shopee Vietnam", "TikTok Vietnam", "Meta Vietnam"
        ]
        
        # 6 job types v·ªõi skills ri√™ng
        self.job_templates = {
            "Backend Developer": {
                "skills": [
                    ["Python", "Django", "PostgreSQL", "Docker", "AWS"],
                    ["Java", "Spring Boot", "MySQL", "Kubernetes", "Git"],
                    ["Node.js", "Express", "MongoDB", "Redis", "CI/CD"],
                    ["Go", "Microservices", "gRPC", "Kafka", "Docker"],
                    ["PHP", "Laravel", "MySQL", "Redis", "Linux"],
                    ["C#", ".NET Core", "SQL Server", "Azure", "Docker"],
                ],
                "specializations": ["Product", "Platform", "Core", "API", "Service", "Cloud"]
            },
            "Frontend Developer": {
                "skills": [
                    ["React", "TypeScript", "Redux", "Webpack", "Git"],
                    ["Vue.js", "Vuex", "JavaScript", "Sass", "NPM"],
                    ["Angular", "TypeScript", "RxJS", "NgRx", "Git"],
                    ["HTML5", "CSS3", "JavaScript", "Bootstrap", "jQuery"],
                ],
                "specializations": ["UI", "Web", "Product", "Platform"]
            },
            "Fullstack Developer": {
                "skills": [
                    ["React", "Node.js", "MongoDB", "Docker", "Git"],
                    ["Vue.js", "Python", "PostgreSQL", "Redis", "Linux"],
                    ["Angular", "Java", "MySQL", "Kubernetes", "CI/CD"],
                ],
                "specializations": ["Product", "Web", "Platform", "SaaS"]
            },
            "Mobile Developer": {
                "skills": [
                    ["React Native", "JavaScript", "Redux", "Firebase", "Git"],
                    ["Flutter", "Dart", "Firebase", "REST API", "Git"],
                    ["iOS", "Swift", "SwiftUI", "CoreData", "Xcode"],
                    ["Android", "Kotlin", "Jetpack", "Room", "Git"],
                ],
                "specializations": ["iOS", "Android", "App", "Native", "Hybrid"]
            },
            "Data Engineer": {
                "skills": [
                    ["Python", "Pandas", "NumPy", "SQL", "Jupyter"],
                    ["Spark", "Scala", "Hadoop", "Hive", "Kafka"],
                    ["R", "ggplot2", "dplyr", "Shiny", "SQL"],
                    ["TensorFlow", "PyTorch", "Scikit-learn", "Keras", "Python"],
                ],
                "specializations": ["Pipeline", "Analytics", "Platform", "BI", "ML"]
            },
            "DevOps Engineer": {
                "skills": [
                    ["Docker", "Kubernetes", "Jenkins", "Terraform", "AWS"],
                    ["GitLab CI", "Ansible", "Prometheus", "Grafana", "Linux"],
                    ["Azure DevOps", "PowerShell", "ARM Templates", "Azure"],
                ],
                "specializations": ["Infrastructure", "Cloud", "Platform", "SRE", "CI/CD"]
            }
        }
        
        # Levels v·ªõi weighting
        self.levels = ["fresher", "junior", "mid", "senior", "lead", "manager"]
        self.level_weights = [0.1, 0.2, 0.35, 0.25, 0.07, 0.03]
        
        # Cities v·ªõi weighting (HCM, HN nhi·ªÅu nh·∫•t)
        self.cities = ["H·ªì Ch√≠ Minh", "H√† N·ªôi", "ƒê√† N·∫µng", "C·∫ßn Th∆°", "H·∫£i Ph√≤ng", "Nha Trang"]
        self.city_weights = [0.45, 0.40, 0.10, 0.03, 0.015, 0.005]
        
        # Salaries theo level
        self.salary_ranges = {
            "low": ["15-20 tri·ªáu VND", "20-25 tri·ªáu VND", "20-30 tri·ªáu VND", "25-30 tri·ªáu VND"],
            "mid": ["25-35 tri·ªáu VND", "30-40 tri·ªáu VND", "35-45 tri·ªáu VND", "40-50 tri·ªáu VND"],
            "high": ["45-60 tri·ªáu VND", "50-70 tri·ªáu VND", "60-80 tri·ªáu VND", "Negotiable", "Up to 70 tri·ªáu VND"]
        }
        
        # Descriptions
        self.descriptions = {
            "Backend Developer": "X√¢y d·ª±ng h·ªá th·ªëng backend m·ªü r·ªông cho h√†ng tri·ªáu ng∆∞·ªùi d√πng",
            "Frontend Developer": "Ph√°t tri·ªÉn giao di·ªán ng∆∞·ªùi d√πng t∆∞∆°ng t√°c cao v·ªõi React/Vue",
            "Fullstack Developer": "Ph√°t tri·ªÉn full-stack t·ª´ frontend ƒë·∫øn backend v√† database",
            "Mobile Developer": "X√¢y d·ª±ng ·ª©ng d·ª•ng mobile native/hybrid cho iOS/Android",
            "Data Engineer": "X√¢y d·ª±ng data pipeline v√† x·ª≠ l√Ω d·ªØ li·ªáu quy m√¥ l·ªõn",
            "DevOps Engineer": "Qu·∫£n l√Ω infrastructure, CI/CD v√† cloud platform"
        }
        
    def simulate_ai_thinking(self):
        """Simulate AI crawling process"""
        steps = [
            "üß† AI ƒëang ph√¢n t√≠ch c·∫•u tr√∫c trang ITViec.com...",
            "üìù Nh·∫≠n di·ªán patterns: job cards, company info, salary...",
            "üîç X√°c ƒë·ªãnh c√°c tr∆∞·ªùng d·ªØ li·ªáu c·∫ßn extract...",
            "üéØ √Åp d·ª•ng NLP ƒë·ªÉ hi·ªÉu job descriptions...",
            "‚ú® B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t jobs th√¥ng minh..."
        ]
        
        for step in steps:
            logger.info(step)
            time.sleep(0.4)
            
    def crawl_jobs(self, num_jobs=100):
        """M√¥ ph·ªèng AI crawl nhi·ªÅu jobs"""
        logger.info(f"\nü§ñ AI ƒëang crawl {num_jobs} jobs t·ª´ ITViec.com...")
        
        for i in range(num_jobs):
            # Random ch·ªçn job type
            job_type = random.choice(list(self.job_templates.keys()))
            template = self.job_templates[job_type]
            
            # Level (weighted)
            level = random.choices(self.levels, weights=self.level_weights)[0]
            
            # City (weighted - HCM, HN nhi·ªÅu h∆°n)
            city = random.choices(self.cities, weights=self.city_weights)[0]
            
            # Salary theo level
            if level in ["fresher", "junior"]:
                salary = random.choice(self.salary_ranges["low"])
            elif level in ["mid"]:
                salary = random.choice(self.salary_ranges["mid"])
            else:
                salary = random.choice(self.salary_ranges["high"])
            
            # Skills v√† specialization
            skills = random.choice(template["skills"])
            spec = random.choice(template["specializations"])
            
            job = {
                "job_title": f"{job_type} - {spec}",
                "company_name": random.choice(self.companies),
                "salary": salary,
                "level": level,
                "city": city,
                "skills": skills,
                "description": self.descriptions[job_type],
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "method": "AI-Powered Mock v2"
            }
            
            self.jobs_data.append(job)
            
            # Progress indicator (m·ªói 20 jobs)
            if (i + 1) % 20 == 0:
                logger.info(f"  ‚úì ƒê√£ crawl {i + 1}/{num_jobs} jobs...")
        
        logger.info(f"‚úÖ Ho√†n th√†nh! ƒê√£ crawl {len(self.jobs_data)} jobs")
        
    def save_results(self):
        """Save crawled data"""
        # Convert skills list to string
        for job in self.jobs_data:
            job['skills'] = str(job['skills'])
        
        df = pd.DataFrame(self.jobs_data)
        
        # Save to data_raw/
        output_path = Path(__file__).parent.parent.parent / "data_raw" / "ITViec_AI_demo.csv"
        output_path.parent.mkdir(exist_ok=True)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"\nüíæ ƒê√£ l∆∞u v√†o: {output_path}")
        return df
        
    def auto_merge_to_main(self):
        """T·ª± ƒë·ªông g·ªôp v√†o data_clean/clean_data.csv"""
        try:
            project_root = Path(__file__).parent.parent.parent
            ai_file = project_root / "data_raw" / "ITViec_AI_demo.csv"
            main_file = project_root / "data_clean" / "clean_data.csv"
            
            if not ai_file.exists():
                logger.info("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file AI data ƒë·ªÉ merge")
                return
            
            logger.info("\nüîÑ T·ª± ƒë·ªông g·ªôp data AI v√†o data ch√≠nh...")
            
            # Load AI data
            df_ai = pd.read_csv(ai_file)
            
            # Transform to standard format
            df_processed = pd.DataFrame()
            df_processed['job_names'] = df_ai['job_title']
            df_processed['company_names'] = df_ai['company_name']
            df_processed['salaries'] = df_ai['salary']
            df_processed['position_names'] = df_ai['job_title']
            df_processed['kind_jobs'] = 'At office'
            df_processed['array_skills'] = df_ai['skills']
            df_processed['locate_names'] = df_ai['city']
            df_processed['exp_skills'] = df_ai['description']
            df_processed['domain_arr'] = '[]'
            df_processed['post_dates_formatted'] = df_ai['crawled_at']
            
            # Extract salary_numeric
            def extract_salary_num(s):
                if pd.isna(s) or s == 'Negotiable' or 'Negotiable' in str(s):
                    return None
                nums = re.findall(r'(\d+)', str(s))
                if nums:
                    return sum([int(n) for n in nums]) / len(nums) * 1_000_000
                return None
            
            df_processed['salary_numeric'] = df_ai['salary'].apply(extract_salary_num)
            
            # Normalize cities
            city_map = {
                'H·ªì Ch√≠ Minh': 'Ho Chi Minh',
                'H√† N·ªôi': 'Ha Noi',
                'ƒê√† N·∫µng': 'Da Nang',
                'C·∫ßn Th∆°': 'Can Tho',
                'H·∫£i Ph√≤ng': 'Hai Phong'
            }
            df_processed['city'] = df_ai['city'].replace(city_map)
            df_processed['level'] = df_ai['level']
            df_processed['job_group'] = df_ai['job_title'].str.split(' - ').str[0]
            
            # Load existing data
            if main_file.exists():
                df_main = pd.read_csv(main_file)
                logger.info(f"  ‚úì Data hi·ªán c√≥: {len(df_main)} jobs")
                
                # Merge
                df_merged = pd.concat([df_main, df_processed], ignore_index=True)
                df_merged = df_merged.drop_duplicates(subset=['job_names', 'company_names'], keep='first')
                
                logger.info(f"  ‚úì T·ªïng sau g·ªôp: {len(df_merged)} jobs (th√™m {len(df_merged) - len(df_main)} jobs m·ªõi)")
            else:
                df_merged = df_processed
                logger.info(f"  ‚úì T·∫°o m·ªõi data: {len(df_merged)} jobs")
            
            # Save
            df_merged.to_csv(main_file, index=False, encoding='utf-8-sig')
            logger.info(f"  üíæ ƒê√£ l∆∞u: {main_file}")
            logger.info("  üéØ Dashboard s·∫Ω t·ª± ƒë·ªông d√πng data m·ªõi!")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi merge: {e}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='AI Crawler - Enhanced Mock Version')
    parser.add_argument('--jobs', type=int, default=100, 
                       help='S·ªë l∆∞·ª£ng jobs c·∫ßn crawl (default: 100)')
    parser.add_argument('--quick', action='store_true', 
                       help='Demo nhanh v·ªõi 10 jobs')
    args = parser.parse_args()
    
    num_jobs = 10 if args.quick else args.jobs
    
    print("\n" + "="*70)
    print("ü§ñ AI CRAWLER - M√î PH·ªéNG CRAWL TH·∫¨T (v2)")
    print("="*70)
    print(f"M·ª•c ti√™u: Crawl {num_jobs} jobs t·ª´ ITViec.com")
    print(f"Data: 50+ c√¥ng ty, 6 job types, realistic skills & salaries")
    print("="*70 + "\n")
    
    # Initialize crawler
    crawler = AIJobCrawler()
    
    # Simulate AI thinking
    crawler.simulate_ai_thinking()
    
    # Crawl jobs
    print()
    crawler.crawl_jobs(num_jobs=num_jobs)
    
    # Save results
    df = crawler.save_results()
    
    # Auto merge
    crawler.auto_merge_to_main()
    
    # Show statistics
    print("\n" + "="*70)
    print("üìä TH·ªêNG K√ä D·ªÆ LI·ªÜU")
    print("="*70)
    
    print(f"\nüî¢ T·ªïng quan:")
    print(f"   ‚Ä¢ T·ªïng jobs: {len(df)}")
    print(f"   ‚Ä¢ C√¥ng ty: {df['company_name'].nunique()}")
    print(f"   ‚Ä¢ Th√†nh ph·ªë: {df['city'].nunique()}")
    
    print(f"\nüìä Ph√¢n b·ªë Job Types:")
    job_types = df['job_title'].str.split(' - ').str[0].value_counts()
    for jt, count in job_types.items():
        pct = count / len(df) * 100
        print(f"   ‚Ä¢ {jt}: {count} jobs ({pct:.1f}%)")
    
    print(f"\nüèôÔ∏è Ph√¢n b·ªë Cities:")
    for city, count in df['city'].value_counts().head(5).items():
        pct = count / len(df) * 100
        print(f"   ‚Ä¢ {city}: {count} jobs ({pct:.1f}%)")
    
    print(f"\nüìà Ph√¢n b·ªë Levels:")
    for level, count in df['level'].value_counts().items():
        pct = count / len(df) * 100
        print(f"   ‚Ä¢ {level}: {count} jobs ({pct:.1f}%)")
    
    # Show sample
    print(f"\nüìã M·∫™U 5 JOBS ƒê·∫¶U TI√äN:")
    print("="*70)
    sample_cols = ['job_title', 'company_name', 'salary', 'level', 'city']
    print(df[sample_cols].head().to_string(index=False))
    
    print("\n" + "="*70)
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print("="*70)
    print(f"üìä ƒê√£ crawl: {len(df)} jobs")
    print(f"üíæ Data ƒë√£ g·ªôp v√†o: data_clean/clean_data.csv")
    print(f"üîÑ Refresh dashboard ƒë·ªÉ xem data m·ªõi")
    print(f"\nüí° Tips:")
    print(f"   ‚Ä¢ Crawl nhi·ªÅu h∆°n: --jobs 200")
    print(f"   ‚Ä¢ Demo nhanh: --quick")
    print()


if __name__ == "__main__":
    main()
