"""
ğŸš€ AI Crawler vá»›i Groq API - MIá»„N PHÃ + Cá»°C NHANH
=================================================
Groq = API miá»…n phÃ­, nhanh hÆ¡n GPT-4, khÃ´ng cáº§n download gÃ¬!

âš ï¸ YÃŠU Cáº¦U:
    - Groq API key (FREE): https://console.groq.com
    - pip install groq

ğŸš€ USAGE:
    python src/crawler/ITViec_AI_groq.py --jobs 20

ğŸ’° CHI PHÃ: MIá»„N PHÃ (free tier: 30 req/min)
â±ï¸ THá»œI GIAN: ~1-2 phÃºt (nhanh!)
"""

import os
import sys
import io
import json
import asyncio
from pathlib import Path
from datetime import datetime
import logging
import pandas as pd
import re
from dotenv import load_dotenv

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

load_dotenv()


async def crawl_with_groq(num_jobs=20):
    """Crawl ITViec báº±ng Playwright + Groq API"""
    
    api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        logger.error("âŒ ChÆ°a cÃ³ Groq API key!")
        logger.info("\nğŸ“ Láº¥y API key miá»…n phÃ­:")
        logger.info("   1. VÃ o: https://console.groq.com")
        logger.info("   2. Sign up (miá»…n phÃ­)")
        logger.info("   3. Táº¡o API key")
        logger.info("   4. ThÃªm vÃ o .env: GROQ_API_KEY=gsk_...")
        return []
    
    logger.info(f"âœ… API key loaded")
    logger.info(f"ğŸš€ Model: Llama 3 70B (qua Groq - cá»±c nhanh!)\n")
    
    try:
        from playwright.async_api import async_playwright
        from groq import Groq
        
        client = Groq(api_key=api_key)
        
        logger.info("ğŸŒ Äang khá»Ÿi Ä‘á»™ng browser...")
        
        async with async_playwright() as p:
            # Launch with stealth mode
            browser = await p.chromium.launch(
                headless=False,  # Show browser Ä‘á»ƒ bypass detection
                args=['--disable-blink-features=AutomationControlled']
            )
            
            # Create page with real user agent
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            # Hide automation
            await page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                })
            """)
            
            logger.info("ğŸ“¡ Äang vÃ o ITViec.com...")
            await page.goto("https://itviec.com/it-jobs", wait_until='domcontentloaded', timeout=30000)
            
            # Wait a bit for page to load
            await page.wait_for_timeout(3000)
            
            # Scroll slowly like human
            logger.info("ğŸ“œ Äang scroll nhÆ° ngÆ°á»i tháº­t...")
            for i in range(3):
                await page.evaluate(f"window.scrollTo(0, {(i+1) * 500})")
                await page.wait_for_timeout(500)
            
            logger.info("ğŸ“¸ Äang láº¥y HTML content...")
            content = await page.content()
            
            # Debug: print HTML length
            logger.info(f"ğŸ“ HTML length: {len(content):,} characters")
            
            await browser.close()
            
            # Extract HTML snippet (larger for better context)
            html_snippet = content[:20000]  # 20K chars for better job extraction
            
            logger.info("ğŸ§  Äang gá»­i HTML cho Groq AI...")
            logger.info("â±ï¸ Äá»£i ~30 giÃ¢y (Groq siÃªu nhanh)...\n")
            
            prompt = f"""You are a web scraping expert. Extract exactly {num_jobs} jobs from this ITViec.com HTML.

FIND job listings in the HTML - they usually have:
- Job titles (h3, h2, or class="job-title")
- Company names (class="company-name" or similar)
- Salary information
- Location/city

For EACH job you find, extract:
- job_title: The position name
- company_name: Company hiring
- salary: Salary range or "Negotiable"
- level: junior/mid/senior/fresher (or guess from title)
- city: Work location
- skills: Programming languages/tech mentioned
- description: Brief job summary

Return a JSON array with {num_jobs} jobs. If you find fewer jobs, return what you found.

Format:
[
  {{
    "job_title": "Backend Developer",
    "company_name": "VNG Corporation",
    "salary": "$1000-2000",
    "level": "mid",
    "city": "Ho Chi Minh",
    "skills": "Python, Django, PostgreSQL",
    "description": "Develop and maintain backend services"
  }}
]

HTML:
{html_snippet}

RETURN ONLY THE JSON ARRAY, NO EXPLANATION."""
            
            # Call Groq API
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",  # Latest free model
                messages=[
                    {
                        "role": "system",
                        "content": "You extract structured data from HTML. Return only valid JSON."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,
                max_tokens=4000
            )
            
            result = response.choices[0].message.content
            logger.info("âœ… Groq Ä‘Ã£ tráº£ vá» káº¿t quáº£!")
            logger.info(f"ğŸ“ Response length: {len(result)} chars")
            
            # Parse JSON with better error handling
            try:
                # Try direct parse first
                jobs = json.loads(result)
            except json.JSONDecodeError:
                # Fallback: extract JSON array
                try:
                    json_start = result.find('[')
                    json_end = result.rfind(']') + 1
                    
                    if json_start != -1 and json_end > json_start:
                        json_str = result[json_start:json_end]
                        jobs = json.loads(json_str)
                    else:
                        logger.error("âŒ KhÃ´ng tÃ¬m tháº¥y JSON")
                        logger.info(f"Response: {result[:500]}")
                        return []
                except Exception as e:
                    logger.error(f"âŒ Parse error: {e}")
                    return []
            
            if isinstance(jobs, list) and len(jobs) > 0:
                    
                    # Add metadata
                    for job in jobs:
                        job['crawled_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        job['method'] = "Playwright + Groq Llama 3.1 70B"
                    
                    logger.info(f"ğŸ“Š ÄÃ£ extract {len(jobs)} jobs!")
                    return jobs
                else:
                    logger.error("âŒ KhÃ´ng tÃ¬m tháº¥y JSON")
                    return []
                    
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Lá»—i parse JSON: {e}")
                logger.info(f"\nğŸ“ Response:\n{result[:500]}...")
                return []
            
    except ImportError as e:
        logger.error(f"\nâŒ Thiáº¿u thÆ° viá»‡n: {e}")
        logger.info("\nğŸ“¦ CÃ i Ä‘áº·t:")
        logger.info("   pip install groq playwright")
        logger.info("   playwright install chromium")
        return []
        
    except Exception as e:
        logger.error(f"\nâŒ Lá»—i: {e}")
        logger.info("\nğŸ’¡ Kiá»ƒm tra:")
        logger.info("   - API key Ä‘Ãºng chÆ°a?")
        logger.info("   - ÄÃ£ sign up Groq chÆ°a?")
        return []


def save_and_merge(jobs_data):
    """Save vÃ  merge vÃ o data chÃ­nh"""
    if len(jobs_data) == 0:
        logger.warning("âš ï¸ KhÃ´ng cÃ³ data")
        return None
    
    df = pd.DataFrame(jobs_data)
    
    # Save
    output_path = Path(__file__).parent.parent.parent / "data_raw" / "ITViec_AI_groq.csv"
    output_path.parent.mkdir(exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    logger.info(f"\nğŸ’¾ ÄÃ£ lÆ°u: {output_path}")
    
    # Merge
    try:
        main_file = Path(__file__).parent.parent.parent / "data_clean" / "clean_data.csv"
        
        if not main_file.exists():
            return df
        
        logger.info("\nğŸ”„ Äang merge...")
        
        df_processed = pd.DataFrame()
        df_processed['job_names'] = df['job_title']
        df_processed['company_names'] = df['company_name']
        df_processed['salaries'] = df['salary']
        df_processed['position_names'] = df['job_title']
        df_processed['kind_jobs'] = 'At office'
        df_processed['array_skills'] = df['skills']
        df_processed['locate_names'] = df['city']
        df_processed['exp_skills'] = df['description']
        df_processed['domain_arr'] = '[]'
        df_processed['post_dates_formatted'] = df['crawled_at']
        
        def extract_sal(s):
            if pd.isna(s) or 'Negotiable' in str(s):
                return None
            nums = re.findall(r'(\d+)', str(s))
            return sum([int(n) for n in nums]) / len(nums) * 1_000_000 if nums else None
        
        df_processed['salary_numeric'] = df['salary'].apply(extract_sal)
        
        city_map = {'Há»“ ChÃ­ Minh': 'Ho Chi Minh', 'HÃ  Ná»™i': 'Ha Noi', 'ÄÃ  Náºµng': 'Da Nang'}
        df_processed['city'] = df['city'].replace(city_map)
        df_processed['level'] = df['level']
        df_processed['job_group'] = df['job_title'].str.split().str[0]
        
        df_main = pd.read_csv(main_file)
        before = len(df_main)
        
        df_merged = pd.concat([df_main, df_processed], ignore_index=True)
        df_merged = df_merged.drop_duplicates(subset=['job_names', 'company_names'])
        
        df_merged.to_csv(main_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"  âœ“ TrÆ°á»›c: {before} jobs")
        logger.info(f"  âœ“ Sau: {len(df_merged)} jobs (+{len(df_merged) - before})")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i merge: {e}")
    
    return df


async def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--jobs', type=int, default=20)
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸš€ AI CRAWLER vá»›i GROQ API - MIá»„N PHÃ + Cá»°C NHANH!")
    print("="*70)
    print(f"Target: {args.jobs} jobs tá»« ITViec.vn")
    print(f"AI: Llama 3.1 70B qua Groq (nhanh hÆ¡n GPT-4)")
    print(f"ğŸ’° Chi phÃ­: $0 (MIá»„N PHÃ)")
    print(f"â±ï¸ Thá»i gian: ~1-2 phÃºt")
    print("="*70 + "\n")
    
    # Crawl
    jobs = await crawl_with_groq(num_jobs=args.jobs)
    
    if len(jobs) == 0:
        logger.error("\nâŒ Crawl tháº¥t báº¡i")
        logger.info("\nğŸ’¡ Láº¥y API key miá»…n phÃ­:")
        logger.info("   https://console.groq.com")
        return
    
    # Save
    df = save_and_merge(jobs)
    
    if df is not None:
        print("\n" + "="*70)
        print("ğŸ“Š Káº¾T QUáº¢")
        print("="*70)
        print(f"\nâœ… Crawled: {len(df)} jobs THáº¬T")
        print(f"ğŸš€ API: Groq (miá»…n phÃ­, nhanh)")
        print(f"ğŸ¢ Companies: {df['company_name'].nunique()}")
        
        print(f"\nğŸ“‹ SAMPLE:")
        cols = ['job_title', 'company_name', 'city']
        print(df[cols].head().to_string(index=False))
        
        print("\n" + "="*70)
        print("âœ… HOÃ€N THÃ€NH - KHÃ”NG Tá»N TIá»€N + Cá»°C NHANH!")
        print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
