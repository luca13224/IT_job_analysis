# üöÄ Quick Start Guide

## C√°ch S·ª≠ D·ª•ng Nhanh

### Option 1: S·ª≠ d·ª•ng Script (Recommended cho Windows)

```bash
# Double-click v√†o file run.bat
# Ho·∫∑c ch·∫°y t·ª´ command line:
run.bat
```

Menu s·∫Ω hi·ªÉn th·ªã c√°c t√πy ch·ªçn:
1. Run Data Processing
2. Run Salary Analysis  
3. Run Skill Analysis
4. Train ML Models
5. **Run Complete Pipeline** ‚≠ê (Ch·∫°y t·∫•t c·∫£)
6. Launch Dashboard
7. Crawl New Data
8. Install Dependencies

### Option 2: Ch·∫°y T·ª´ng Module

#### 1. X·ª≠ l√Ω d·ªØ li·ªáu
```bash
python src/data_processing/processor.py
```

#### 2. Ph√¢n t√≠ch l∆∞∆°ng
```bash
python src/analysis/salary_analytics.py
```

#### 3. Ph√¢n t√≠ch k·ªπ nƒÉng
```bash
python src/nlp/skill_analyzer.py
```

#### 4. Train ML models
```bash
python src/ml_models/salary_prediction.py
```

#### 5. Ch·∫°y to√†n b·ªô pipeline
```bash
python main.py
```

#### 6. Launch dashboard
```bash
streamlit run src/visualization/dashboard.py
```

### Option 3: D√πng Notebook

M·ªü Jupyter Notebook:
```bash
jupyter notebook
```

Sau ƒë√≥ m·ªü c√°c notebook trong th∆∞ m·ª•c `notebooks/`:
- `eda.ipynb` - Exploratory Data Analysis
- `cleanning_data.ipynb` - Data Cleaning
- `crawling_test.ipynb` - Test Crawler

---

## Workflow ƒê·ªÅ Xu·∫•t

### L·∫ßn ƒë·∫ßu setup:

1. **C√†i ƒë·∫∑t dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Crawl d·ªØ li·ªáu** (n·∫øu ch∆∞a c√≥)
   ```bash
   python src/crawler/ITViec_crawling.py
   ```
   - ƒêƒÉng nh·∫≠p ITViec khi browser m·ªü
   - Nh·∫•n ENTER ƒë·ªÉ b·∫Øt ƒë·∫ßu crawl

3. **Ch·∫°y complete pipeline**
   ```bash
   python main.py
   ```
   Ho·∫∑c d√πng `run.bat` ‚Üí ch·ªçn option 5

4. **Xem k·∫øt qu·∫£ trong Dashboard**
   ```bash
   streamlit run src/visualization/dashboard.py
   ```
   Ho·∫∑c d√πng `run.bat` ‚Üí ch·ªçn option 6

### S·ª≠ d·ª•ng h√†ng ng√†y:

1. **Update d·ªØ li·ªáu m·ªõi**
   - Ch·∫°y crawler ƒë·ªÉ l·∫•y jobs m·ªõi

2. **Xem ph√¢n t√≠ch**
   - M·ªü dashboard ƒë·ªÉ xem insights

3. **D·ª± ƒëo√°n l∆∞∆°ng**
   ```python
   from src.ml_models.salary_prediction import SalaryPredictor
   
   predictor = SalaryPredictor()
   predictor.load_model()
   
   result = predictor.predict_salary(
       job_group='Backend Developer',
       level='senior',
       city='Ho Chi Minh',
       skills=['python', 'django', 'aws']
   )
   
   print(f"Predicted: {result['predicted_salary_m']:.2f}M VND")
   ```

---

## K·∫øt Qu·∫£ Output

Sau khi ch·∫°y pipeline, b·∫°n s·∫Ω c√≥:

### Trong th∆∞ m·ª•c `outputs/`:
- `salary_distribution.png` - Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi l∆∞∆°ng
- `salary_trends.png` - Xu h∆∞·ªõng l∆∞∆°ng theo th·ªùi gian
- `feature_importance.png` - C√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng l∆∞∆°ng
- `model_comparison.png` - So s√°nh c√°c ML models
- `salary_analysis_report.txt` - B√°o c√°o chi ti·∫øt
- `skill_trends.csv` - Top skills
- `skill_cooccurrence.csv` - Skills xu·∫•t hi·ªán c√πng nhau

### Trong th∆∞ m·ª•c `models/`:
- `salary_predictor.pkl` - ML model ƒë√£ train

### Trong th∆∞ m·ª•c `data_clean/`:
- `clean_data.csv` - D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω

---

## Tips & Tricks

### 1. Ch·∫°y nhanh m·ªôt ph·∫ßn c·ªßa pipeline
```python
# Ch·ªâ ch·∫°y data processing
from src.data_processing.processor import DataProcessor
processor = DataProcessor()
processor.process_pipeline()
```

### 2. Load model ƒë√£ train ƒë·ªÉ d·ª± ƒëo√°n
```python
from src.ml_models.salary_prediction import SalaryPredictor
predictor = SalaryPredictor()
predictor.load_model()  # Load model ƒë√£ train
```

### 3. Filter data tr∆∞·ªõc khi ph√¢n t√≠ch
```python
import pandas as pd

df = pd.read_csv('data_clean/clean_data.csv')

# Ch·ªâ l·∫•y Backend Developer
backend_df = df[df['job_group'] == 'Backend Developer']

# Ch·ªâ l·∫•y jobs ·ªü HCM
hcm_df = df[df['city'] == 'Ho Chi Minh']

# Ch·ªâ l·∫•y senior level
senior_df = df[df['level'] == 'senior']
```

### 4. Export k·∫øt qu·∫£ sang Excel
```python
import pandas as pd

df = pd.read_csv('data_clean/clean_data.csv')

# Export sang Excel v·ªõi nhi·ªÅu sheets
with pd.ExcelWriter('analysis_results.xlsx') as writer:
    df.to_excel(writer, sheet_name='All Data', index=False)
    
    # Top paying jobs
    top_salary = df.nlargest(100, 'salary_numeric')
    top_salary.to_excel(writer, sheet_name='Top Salary', index=False)
    
    # By city
    for city in df['city'].unique():
        city_df = df[df['city'] == city]
        city_df.to_excel(writer, sheet_name=city, index=False)
```

---

## Troubleshooting

### L·ªói: Module not found
```bash
# C√†i l·∫°i dependencies
pip install -r requirements.txt --force-reinstall
```

### L·ªói: File not found
```bash
# Ch·∫°y data processing tr∆∞·ªõc
python src/data_processing/processor.py
```

### Dashboard ch·∫°y ch·∫≠m
```bash
# Gi·∫£m s·ªë l∆∞·ª£ng data
df = df.sample(1000)  # Ch·ªâ l·∫•y 1000 records
```

### Crawler kh√¥ng ho·∫°t ƒë·ªông
```bash
# Update webdriver
pip install --upgrade webdriver-manager
```

---

## Next Steps

Sau khi l√†m quen v·ªõi d·ª± √°n:

1. **Customize dashboard** - Th√™m charts ri√™ng c·ªßa b·∫°n
2. **Add new features** - Th√™m features m·ªõi v√†o ML model
3. **Expand crawlers** - Th√™m crawler cho TopCV, VietnamWorks
4. **Deploy** - Deploy dashboard l√™n Streamlit Cloud

Happy analyzing! üéâ
